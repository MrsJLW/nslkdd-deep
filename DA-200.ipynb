{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = np.genfromtxt('resources/train.csv', dtype=np.float32, delimiter=',')\n",
    "train_targets = train_dataset[:,-1]\n",
    "train_targets = np.array([1 if x == 2 else 0 for x in train_targets])\n",
    "test_dataset = np.genfromtxt('resources/test.csv', dtype=np.float32, delimiter=',')\n",
    "test_targets = test_dataset[:,-1]\n",
    "test_targets = np.array([1 if x == 2 else 0 for x in test_targets])\n",
    "combined_dataset=np.concatenate((train_dataset[:,:-1], test_dataset[:,:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58630, 67343])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "numeric_indices = []\n",
    "for i in range(combined_dataset.shape[1]):\n",
    "    n_unique = len(np.unique(combined_dataset[:, i]))\n",
    "    numeric_indices.append(n_unique > 2)\n",
    "print (len(numeric_indices))\n",
    "numeric_indices = np.array(numeric_indices, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_dataset[:, numeric_indices] = StandardScaler().fit_transform(\n",
    "                                        combined_dataset[:, numeric_indices]\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 1500\n",
    "num_feature = 122\n",
    "num_labels = 2\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    target = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "    n_input = num_feature\n",
    "    n_hidden_1 = 200\n",
    "    n_hidden_2 = 50\n",
    "    n_hidden_3 = 2\n",
    "    \n",
    "    inputs = {'l1': tf.placeholder(tf.float32, shape=(None, num_feature)),\n",
    "              'l2': tf.placeholder(tf.float32, shape=(None, n_hidden_1)),\n",
    "              'l3': tf.placeholder(tf.float32, shape=(None, n_hidden_2))}\n",
    "    \n",
    "    targets = {'l1': tf.placeholder(tf.float32, shape=(None, num_feature)),\n",
    "               'l2': tf.placeholder(tf.float32, shape=(None, n_hidden_1)),\n",
    "               'l3': tf.placeholder(tf.float32, shape=(None, num_labels))}\n",
    "\n",
    "    weights = {'l1': tf.Variable(tf.truncated_normal([n_input, n_hidden_1], dtype=tf.float32)),\n",
    "               'l2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], dtype=tf.float32)),\n",
    "               'l3': tf.Variable(tf.truncated_normal([n_hidden_2, num_labels], dtype=tf.float32))}\n",
    "\n",
    "    enc_biases = {'l1': tf.Variable(tf.truncated_normal([n_hidden_1], dtype=tf.float32)),\n",
    "                  'l2': tf.Variable(tf.truncated_normal([n_hidden_2], dtype=tf.float32)),\n",
    "                  'l3': tf.Variable(tf.truncated_normal([num_labels], dtype=tf.float32))}\n",
    "    \n",
    "    dec_biases = {'l1': tf.Variable(tf.truncated_normal([n_input], dtype=tf.float32)),\n",
    "                  'l2': tf.Variable(tf.truncated_normal([n_hidden_1], dtype=tf.float32))}\n",
    "    \n",
    "    def encode(layer_id):\n",
    "        return tf.add(tf.matmul(inputs[layer_id], weights[layer_id]), enc_biases[layer_id])\n",
    "    \n",
    "    def decode(layer, layer_id):\n",
    "        return tf.add(tf.matmul(layer, tf.transpose(weights[layer_id])), dec_biases[layer_id])\n",
    "    \n",
    "    def get_loss(inp, out):\n",
    "        difference = tf.sub(inp, out) \n",
    "        return tf.sqrt(tf.reduce_mean(tf.square(difference)))   \n",
    "\n",
    "    def encoder_loss(layer_id):\n",
    "        enc = encode(layer_id)\n",
    "        dec = decode(enc, layer_id)\n",
    "        loss = get_loss(targets[layer_id], dec)\n",
    "        return loss\n",
    "    \n",
    "    def compute_logit(layer_id):\n",
    "        logits = tf.nn.tanh(encode(layer_id))\n",
    "        logits = tf.add(tf.matmul(logits, wo), bo)\n",
    "        return logits\n",
    "    \n",
    "    def logit_loss(logits, layer_id):\n",
    "        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits, targets[layer_id]))\n",
    "    \n",
    "    def full_compute():\n",
    "        logits = inputs['l1']\n",
    "        for layer_id in ['l1', 'l2', 'l3']:\n",
    "            logits = tf.add(tf.matmul(logits, weights[layer_id]), enc_biases[layer_id])\n",
    "        return logits\n",
    "        \n",
    "    def get_sigmoid(logits):\n",
    "        return tf.nn.sigmoid(logits)\n",
    "    \n",
    "    loss_l1 = encoder_loss('l1')\n",
    "    optimizer_l1 = tf.train.AdamOptimizer(0.001).minimize(loss_l1)\n",
    "    loss_l2 = encoder_loss('l2')\n",
    "    optimizer_l2 = tf.train.AdamOptimizer(0.001).minimize(loss_l2)\n",
    "    logits = full_compute()\n",
    "    loss_l3 = logit_loss(logits, 'l3')\n",
    "    optimizer_l3 = tf.train.AdamOptimizer(0.001).minimize(loss_l3 + 0.001*tf.nn.l2_loss(weights['l3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "500 : 86.2553\n",
      "1000 : 36.645\n",
      "1500 : 16.2874\n",
      "2000 : 6.86966\n",
      "2500 : 2.96181\n",
      "3000 : 1.29328\n",
      "3500 : 0.580485\n",
      "4000 : 0.379386\n",
      "4500 : 0.318181\n",
      "5000 : 0.316957\n",
      "5500 : 0.29658\n",
      "6000 : 0.323883\n",
      "6500 : 0.307271\n",
      "7000 : 0.290413\n",
      "7500 : 0.324015\n",
      "8000 : 0.314294\n",
      "8500 : 0.28926\n",
      "9000 : 0.296604\n",
      "9500 : 0.298376\n",
      "10000 : 0.315302\n"
     ]
    }
   ],
   "source": [
    "num_steps = 8500\n",
    "randomized_dataset = combined_dataset.copy()\n",
    "np.random.shuffle(randomized_dataset)\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    for step in range(1, num_steps + 1):\n",
    "        offset = (step * batch_size) % (randomized_dataset.shape[0] - batch_size)\n",
    "        batch_data = randomized_dataset[offset:(offset + batch_size), :]\n",
    "        noisy_batch_data = batch_data.copy()\n",
    "        for i, point in enumerate(noisy_batch_data):\n",
    "            noisy_batch_data[i, :] = point + np.random.normal(0, 1, noisy_batch_data.shape[1])\n",
    "        feed_dict = {inputs['l1']: noisy_batch_data, targets['l1']: batch_data}\n",
    "        _, l = session.run([optimizer_l1, loss_l1], feed_dict=feed_dict)\n",
    "        if step % 500 == 0:\n",
    "            print step, ':', l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "1000 : 79.9693\n",
      "2000 : 50.1782\n",
      "3000"
     ]
    }
   ],
   "source": [
    "num_steps = 18000\n",
    "preds = []\n",
    "randomized_dataset = combined_dataset.copy()\n",
    "np.random.shuffle(randomized_dataset)\n",
    "new_input=[]\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    encoder_op = encode('l1')\n",
    "    encoded_randomized_dataset = encoder_op.eval(feed_dict={inputs['l1']: randomized_dataset})\n",
    "    for step in range(1, num_steps + 1):\n",
    "        offset = (step * batch_size) % (randomized_dataset.shape[0] - batch_size)\n",
    "        batch_data = encoded_randomized_dataset[offset:(offset + batch_size), :]\n",
    "        noisy_batch_data = batch_data.copy()\n",
    "        for i, point in enumerate(noisy_batch_data):\n",
    "            noisy_batch_data[i, :] = point + np.random.normal(0, 1, noisy_batch_data.shape[1])\n",
    "        feed_dict = {inputs['l2']: noisy_batch_data, targets['l2']: batch_data}\n",
    "        _, l = session.run([optimizer_l2, loss_l2], feed_dict=feed_dict)\n",
    "        if step % 1000 == 0:\n",
    "            print step, ':', l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "1 : 57.932\n",
      "2 : 56.0853\n",
      "3 : 50.5863\n",
      "4 : 46.7615\n",
      "5 : 41.7852\n",
      "6 : 40.8184\n",
      "7 : 40.1066\n",
      "8 : 39.5069\n",
      "9 : 34.528\n",
      "10 : 35.5638\n",
      "11 : 36.0201\n",
      "12 : 31.2986\n",
      "13 : 30.3697\n",
      "14 : 22.598\n",
      "15 : 27.5842\n",
      "16 : 23.7671\n",
      "17 : 27.3785\n",
      "18 : 19.7599\n",
      "19 : 16.8592\n",
      "20 : 20.5611\n",
      "21 : 22.1059\n",
      "22 : 19.4908\n",
      "23 : 19.7528\n",
      "24 : 18.9394\n",
      "25 : 14.1991\n",
      "26 : 14.8422\n",
      "27 : 13.1288\n",
      "28 : 18.2879\n",
      "29 : 20.5551\n",
      "30 : 16.562\n",
      "31 : 13.8016\n",
      "32 : 16.4818\n",
      "33 : 13.8816\n",
      "34 : 12.9311\n",
      "35 : 12.6798\n",
      "36 : 11.5377\n",
      "37 : 10.5968\n",
      "38 : 12.1328\n",
      "39 : 12.4286\n",
      "40 : 13.087\n",
      "41 : 10.0677\n",
      "42 : 9.89639\n",
      "43 : 9.85662\n",
      "44 : 9.51702\n",
      "45 : 9.87033\n",
      "46 : 11.9953\n",
      "47 : 8.34478\n",
      "48 : 8.20797\n",
      "49 : 7.98213\n",
      "50 : 8.61769\n",
      "51 : 8.65382\n",
      "52 : 7.28106\n",
      "53 : 5.75337\n",
      "54 : 7.16362\n",
      "55 : 8.79215\n",
      "56 : 6.50359\n",
      "57 : 6.71431\n",
      "58 : 9.2741\n",
      "59 : 5.79919\n",
      "60 : 6.46822\n",
      "61 : 7.93393\n",
      "62 : 5.66905\n",
      "63 : 6.59138\n",
      "64 : 6.72521\n",
      "65 : 5.57835\n",
      "66 : 7.70327\n",
      "67 : 4.98589\n",
      "68 : 7.75177\n",
      "69 : 5.21251\n",
      "70 : 5.04132\n",
      "71 : 5.16483\n",
      "72 : 4.34731\n",
      "73 : 4.92371\n",
      "74 : 5.75147\n",
      "75 : 4.69978\n",
      "76 : 4.62378\n",
      "77 : 5.02925\n",
      "78 : 4.82921\n",
      "79 : 4.47644\n",
      "80 : 5.24507\n",
      "81 : 4.08719\n",
      "82 : 4.77736\n",
      "83 : 5.18656\n",
      "84 : 5.11239\n",
      "85 : 3.58844\n",
      "86 : 4.03117\n",
      "87 : 3.18475\n",
      "88 : 3.70754\n",
      "89 : 2.45026\n",
      "90 : 3.53373\n",
      "91 : 5.2246\n",
      "92 : 2.9029\n",
      "93 : 3.45257\n",
      "94 : 3.5347\n",
      "95 : 3.5983\n",
      "96 : 3.39174\n",
      "97 : 2.8066\n",
      "98 : 2.50971\n",
      "99 : 3.72755\n",
      "100 : 3.00549\n",
      "101 : 2.70182\n",
      "102 : 2.44363\n",
      "103 : 2.611\n",
      "104 : 3.4224\n",
      "105 : 2.6413\n",
      "106 : 2.38638\n",
      "107 : 2.45102\n",
      "108 : 2.01856\n",
      "109 : 2.82359\n",
      "110 : 2.58214\n",
      "111 : 3.97174\n",
      "112 : 4.42138\n",
      "113 : 2.00676\n",
      "114 : 2.03195\n",
      "115 : 3.52767\n",
      "116 : 2.15186\n",
      "117 : 2.1909\n",
      "118 : 2.42071\n",
      "119 : 1.99461\n",
      "120 : 1.76453\n",
      "121 : 2.24953\n",
      "122 : 2.23511\n",
      "123 : 3.00772\n",
      "124 : 2.98499\n",
      "125 : 1.71662\n",
      "126 : 2.1107\n",
      "127 : 1.57762\n",
      "128 : 1.61286\n",
      "129 : 3.22166\n",
      "130 : 1.81904\n",
      "131 : 1.75024\n",
      "132 : 1.70861\n",
      "133 : 1.52993\n",
      "134 : 2.18715\n",
      "135 : 1.40807\n",
      "136 : 1.33285\n",
      "137 : 2.05968\n",
      "138 : 1.52526\n",
      "139 : 1.82246\n",
      "140 : 1.34379\n",
      "141 : 1.86944\n",
      "142 : 1.75281\n",
      "143 : 1.10339\n",
      "144 : 2.01081\n",
      "145 : 1.40515\n",
      "146 : 1.91719\n",
      "147 : 1.32617\n",
      "148 : 1.28826\n",
      "149 : 1.46237\n",
      "150 : 1.4044\n",
      "151 : 1.89026\n",
      "152 : 1.58393\n",
      "153 : 0.997149\n",
      "154 : 1.16549\n",
      "155 : 1.36034\n",
      "156 : 1.40221\n",
      "157 : 1.51016\n",
      "158 : 1.48428\n",
      "159 : 1.09547\n",
      "160 : 1.34975\n",
      "161 : 1.31433\n",
      "162 : 1.31781\n",
      "163 : 1.54325\n",
      "164 : 1.02018\n",
      "165 : 1.47519\n",
      "166 : 1.56135\n",
      "167 : 1.81887\n",
      "168 : 1.11988\n",
      "169 : 1.11817\n",
      "170 : 0.992193\n",
      "171 : 1.20913\n",
      "172 : 0.833068\n",
      "173 : 1.13039\n",
      "174 : 2.33061\n",
      "175 : 1.09994\n",
      "176 : 1.09968\n",
      "177 : 1.37866\n",
      "178 : 1.27501\n",
      "179 : 1.22581\n",
      "180 : 1.20446\n",
      "181 : 0.905157\n",
      "182 : 1.81883\n",
      "183 : 1.01796\n",
      "184 : 0.857783\n",
      "185 : 0.8992\n",
      "186 : 1.11391\n",
      "187 : 1.64183\n",
      "188 : 1.01022\n",
      "189 : 1.03866\n",
      "190 : 1.08236\n",
      "191 : 0.801566\n",
      "192 : 1.12618\n",
      "193 : 1.08142\n",
      "194 : 1.60864\n",
      "195 : 1.90697\n",
      "196 : 0.872164\n",
      "197 : 0.777029\n",
      "198 : 1.66456\n",
      "199 : 0.863244\n",
      "200 : 0.917411\n",
      "201 : 1.00204\n",
      "202 : 0.839338\n",
      "203 : 0.709501\n",
      "204 : 0.908018\n",
      "205 : 0.974517\n",
      "206 : 1.14583\n",
      "207 : 0.792182\n",
      "208 : 0.816309\n",
      "209 : 0.978421\n",
      "210 : 0.742143\n",
      "211 : 0.695669\n",
      "212 : 1.50916\n",
      "213 : 0.767103\n",
      "214 : 0.690848\n",
      "215 : 0.730746\n",
      "216 : 0.626286\n",
      "217 : 1.0272\n",
      "218 : 0.569284\n",
      "219 : 0.518269\n",
      "220 : 1.04292\n",
      "221 : 0.617502\n",
      "222 : 0.722465\n",
      "223 : 0.605654\n",
      "224 : 0.828232\n",
      "225 : 1.06874\n",
      "226 : 0.478773\n",
      "227 : 0.864011\n",
      "228 : 0.661412\n",
      "229 : 0.827909\n",
      "230 : 0.569268\n",
      "231 : 0.533174\n",
      "232 : 0.675283\n",
      "233 : 0.662984\n",
      "234 : 0.841661\n",
      "235 : 0.671378\n",
      "236 : 0.439717\n",
      "237 : 0.435735\n",
      "238 : 0.572849\n",
      "239 : 0.558332\n",
      "240 : 0.733787\n",
      "241 : 0.629676\n",
      "242 : 0.49957\n",
      "243 : 0.522434\n",
      "244 : 0.607513\n",
      "245 : 0.516705\n",
      "246 : 0.674977\n",
      "247 : 0.476461\n",
      "248 : 0.775994\n",
      "249 : 0.449555\n",
      "250 : 0.917687\n",
      "251 : 0.462145\n",
      "252 : 0.479909\n",
      "253 : 0.412076\n",
      "254 : 0.517781\n",
      "255 : 0.418241\n",
      "256 : 0.53178\n",
      "257 : 1.35307\n",
      "258 : 0.464229\n",
      "259 : 0.484838\n",
      "260 : 0.611758\n",
      "261 : 0.561169\n",
      "262 : 0.478368\n",
      "263 : 0.551379\n",
      "264 : 0.367326\n",
      "265 : 1.11785\n",
      "266 : 0.479645\n",
      "267 : 0.296864\n",
      "268 : 0.363602\n",
      "269 : 0.587585\n",
      "270 : 0.836595\n",
      "271 : 0.451134\n",
      "272 : 0.511024\n",
      "273 : 0.523383\n",
      "274 : 0.341496\n",
      "275 : 0.516876\n",
      "276 : 0.470051\n",
      "277 : 0.537599\n",
      "278 : 0.360842\n",
      "279 : 0.455465\n",
      "280 : 0.355723\n",
      "281 : 0.705813\n",
      "282 : 0.454042\n",
      "283 : 0.422667\n",
      "284 : 0.463904\n",
      "285 : 0.402386\n",
      "286 : 0.359354\n",
      "287 : 0.398206\n",
      "288 : 0.369486\n",
      "289 : 0.442872\n",
      "290 : 0.364076\n",
      "291 : 0.379123\n",
      "292 : 0.492967\n",
      "293 : 0.447694\n",
      "294 : 0.355691\n",
      "295 : 0.623815\n",
      "296 : 0.402779\n",
      "297 : 0.34134\n",
      "298 : 0.358802\n",
      "299 : 0.376896\n",
      "300 : 0.464481\n",
      "301 : 0.342596\n",
      "302 : 0.30968\n",
      "303 : 0.594874\n",
      "304 : 0.307715\n",
      "305 : 0.337921\n",
      "306 : 0.373502\n",
      "307 : 0.34646\n",
      "308 : 0.589946\n",
      "309 : 0.205016\n",
      "310 : 0.494758\n",
      "311 : 0.393783\n",
      "312 : 0.398066\n",
      "313 : 0.304388\n",
      "314 : 0.430272\n",
      "315 : 0.415548\n",
      "316 : 0.358416\n",
      "317 : 0.454065\n",
      "318 : 0.373356\n",
      "319 : 0.262221\n",
      "320 : 0.221486\n",
      "321 : 0.275438\n",
      "322 : 0.25674\n",
      "323 : 0.448574\n",
      "324 : 0.361729\n",
      "325 : 0.251287\n",
      "326 : 0.340517\n",
      "327 : 0.369332\n",
      "328 : 0.264173\n",
      "329 : 0.449202\n",
      "330 : 0.352014\n",
      "331 : 0.497715\n",
      "332 : 0.254467\n",
      "333 : 0.585839\n",
      "334 : 0.325279\n",
      "335 : 0.312409\n",
      "336 : 0.246579\n",
      "337 : 0.307966\n",
      "338 : 0.266915\n",
      "339 : 0.349817\n",
      "340 : 0.939081\n",
      "341 : 0.281405\n",
      "342 : 0.30364\n",
      "343 : 0.37486\n",
      "344 : 0.344717\n",
      "345 : 0.341459\n",
      "346 : 0.387086\n",
      "347 : 0.276715\n",
      "348 : 0.745296\n",
      "349 : 0.346505\n",
      "350 : 0.147547\n",
      "351 : 0.228508\n",
      "352 : 0.397642\n",
      "353 : 0.427858\n",
      "354 : 0.292618\n",
      "355 : 0.336302\n",
      "356 : 0.323458\n",
      "357 : 0.231581\n",
      "358 : 0.335019\n",
      "359 : 0.305581\n",
      "360 : 0.382315\n",
      "361 : 0.231941\n",
      "362 : 0.290435\n",
      "363 : 0.261343\n",
      "364 : 0.461719\n",
      "365 : 0.326401\n",
      "366 : 0.2757\n",
      "367 : 0.337148\n",
      "368 : 0.260845\n",
      "369 : 0.242916\n",
      "370 : 0.290424\n",
      "371 : 0.209653\n",
      "372 : 0.339418\n",
      "373 : 0.22909\n",
      "374 : 0.255586\n",
      "375 : 0.351046\n",
      "376 : 0.354947\n",
      "377 : 0.264584\n",
      "378 : 0.384289\n",
      "379 : 0.305951\n",
      "380 : 0.234625\n",
      "381 : 0.243592\n",
      "382 : 0.303263\n",
      "383 : 0.304414\n",
      "384 : 0.306395\n",
      "385 : 0.222662\n",
      "386 : 0.437057\n",
      "387 : 0.198753\n",
      "388 : 0.247885\n",
      "389 : 0.280192\n",
      "390 : 0.236084\n",
      "391 : 0.322901\n",
      "392 : 0.157875\n",
      "393 : 0.369842\n",
      "394 : 0.308985\n",
      "395 : 0.260356\n",
      "396 : 0.222996\n",
      "397 : 0.332551\n",
      "398 : 0.333265\n",
      "399 : 0.302426\n",
      "400 : 0.319428\n",
      "401 : 0.245609\n",
      "402 : 0.198715\n",
      "403 : 0.168522\n",
      "404 : 0.191239\n",
      "405 : 0.182898\n",
      "406 : 0.349213\n",
      "407 : 0.308117\n",
      "408 : 0.180767\n",
      "409 : 0.252724\n",
      "410 : 0.269229\n",
      "411 : 0.201899\n",
      "412 : 0.336576\n",
      "413 : 0.275164\n",
      "414 : 0.408731\n",
      "415 : 0.19569\n",
      "416 : 0.474195\n",
      "417 : 0.217916\n",
      "418 : 0.259058\n",
      "419 : 0.189577\n",
      "420 : 0.214961\n",
      "421 : 0.210829\n",
      "422 : 0.275461\n",
      "423 : 0.681298\n",
      "424 : 0.205557\n",
      "425 : 0.240494\n",
      "426 : 0.276884\n",
      "427 : 0.276808\n",
      "428 : 0.273885\n",
      "429 : 0.297798\n",
      "430 : 0.227585\n",
      "431 : 0.509977\n",
      "432 : 0.26957\n",
      "433 : 0.11435\n",
      "434 : 0.175845\n",
      "435 : 0.306054\n",
      "436 : 0.428992\n",
      "437 : 0.224308\n",
      "438 : 0.259174\n",
      "439 : 0.242444\n",
      "440 : 0.181056\n",
      "441 : 0.263358\n",
      "442 : 0.240327\n",
      "443 : 0.281058\n",
      "444 : 0.192336\n",
      "445 : 0.210506\n",
      "446 : 0.221742\n",
      "447 : 0.407006\n",
      "448 : 0.268224\n",
      "449 : 0.208811\n",
      "450 : 0.261658\n",
      "451 : 0.192356\n",
      "452 : 0.195908\n",
      "453 : 0.238718\n",
      "454 : 0.165852\n",
      "455 : 0.271498\n",
      "456 : 0.18747\n",
      "457 : 0.220919\n",
      "458 : 0.292874\n",
      "459 : 0.284546\n",
      "460 : 0.257417\n",
      "461 : 0.297095\n",
      "462 : 0.238359\n",
      "463 : 0.185181\n",
      "464 : 0.197591\n",
      "465 : 0.261739\n",
      "466 : 0.253703\n",
      "467 : 0.253283\n",
      "468 : 0.17694\n",
      "469 : 0.369354\n",
      "470 : 0.154964\n",
      "471 : 0.204714\n",
      "472 : 0.220907\n",
      "473 : 0.180386\n",
      "474 : 0.259937\n",
      "475 : 0.126179\n",
      "476 : 0.312304\n",
      "477 : 0.252372\n",
      "478 : 0.207487\n",
      "479 : 0.196182\n",
      "480 : 0.321733\n",
      "481 : 0.26825\n",
      "482 : 0.220436\n",
      "483 : 0.263003\n",
      "484 : 0.215035\n",
      "485 : 0.165958\n",
      "486 : 0.152934\n",
      "487 : 0.143457\n",
      "488 : 0.149087\n",
      "489 : 0.295152\n",
      "490 : 0.226239\n",
      "491 : 0.142894\n",
      "492 : 0.2142\n",
      "493 : 0.210405\n",
      "494 : 0.202521\n",
      "495 : 0.225766\n",
      "496 : 0.213107\n",
      "497 : 0.334945\n",
      "498 : 0.175508\n",
      "499 : 0.366237\n",
      "500 : 0.200912\n",
      "501 : 0.218956\n",
      "502 : 0.179666\n",
      "503 : 0.141444\n",
      "504 : 0.18257\n",
      "505 : 0.242114\n",
      "506 : 0.508523\n",
      "507 : 0.169478\n",
      "508 : 0.204702\n",
      "509 : 0.240088\n",
      "510 : 0.233666\n",
      "511 : 0.21184\n",
      "512 : 0.251795\n",
      "513 : 0.197063\n",
      "514 : 0.344879\n",
      "515 : 0.223205\n",
      "516 : 0.107906\n",
      "517 : 0.145559\n",
      "518 : 0.24893\n",
      "519 : 0.340414\n",
      "520 : 0.190728\n",
      "521 : 0.22241\n",
      "522 : 0.205148\n",
      "523 : 0.145942\n",
      "524 : 0.218699\n",
      "525 : 0.199144\n",
      "526 : 0.225994\n",
      "527 : 0.153614\n",
      "528 : 0.176069\n",
      "529 : 0.19365\n",
      "530 : 0.295215\n",
      "531 : 0.226365\n",
      "532 : 0.177308\n",
      "533 : 0.216182\n",
      "534 : 0.149224\n",
      "535 : 0.170494\n",
      "536 : 0.208565\n",
      "537 : 0.150516\n",
      "538 : 0.212129\n",
      "539 : 0.149467\n",
      "540 : 0.175794\n",
      "541 : 0.209422\n",
      "542 : 0.239532\n",
      "543 : 0.230053\n",
      "544 : 0.155981\n",
      "545 : 0.190213\n",
      "546 : 0.164942\n",
      "547 : 0.140817\n",
      "548 : 0.224571\n",
      "549 : 0.244431\n",
      "550 : 0.205717\n",
      "551 : 0.154725\n",
      "552 : 0.438785\n",
      "553 : 0.134803\n",
      "554 : 0.173327\n",
      "555 : 0.201074\n",
      "556 : 0.137692\n",
      "557 : 0.223707\n",
      "558 : 0.134734\n",
      "559 : 0.260029\n",
      "560 : 0.212567\n",
      "561 : 0.182205\n",
      "562 : 0.162256\n",
      "563 : 0.283024\n",
      "564 : 0.241768\n",
      "565 : 0.256394\n",
      "566 : 0.207149\n",
      "567 : 0.171759\n",
      "568 : 0.14128\n",
      "569 : 0.120141\n",
      "570 : 0.113255\n",
      "571 : 0.128279\n",
      "572 : 0.247775\n",
      "573 : 0.184061\n",
      "574 : 0.121843\n",
      "575 : 0.175077\n",
      "576 : 0.176251\n",
      "577 : 0.16748\n",
      "578 : 0.183153\n",
      "579 : 0.160422\n",
      "580 : 0.278672\n",
      "581 : 0.142787\n",
      "582 : 0.310133\n",
      "583 : 0.149508\n",
      "584 : 0.177875\n",
      "585 : 0.178123\n",
      "586 : 0.10917\n",
      "587 : 0.154884\n",
      "588 : 0.20283\n",
      "589 : 0.36721\n",
      "590 : 0.146665\n",
      "591 : 0.168608\n",
      "592 : 0.203389\n",
      "593 : 0.192424\n",
      "594 : 0.191457\n",
      "595 : 0.215716\n",
      "596 : 0.173203\n",
      "597 : 0.239608\n",
      "598 : 0.191644\n",
      "599 : 0.0966464\n",
      "600 : 0.124554\n",
      "601 : 0.205842\n",
      "602 : 0.370508\n",
      "603 : 0.161655\n",
      "604 : 0.181314\n",
      "605 : 0.175615\n",
      "606 : 0.130668\n",
      "607 : 0.186171\n",
      "608 : 0.170839\n",
      "609 : 0.177151\n",
      "610 : 0.140326\n",
      "611 : 0.145724\n",
      "612 : 0.198738\n",
      "613 : 0.242584\n",
      "614 : 0.198329\n",
      "615 : 0.15344\n",
      "616 : 0.177177\n",
      "617 : 0.134651\n",
      "618 : 0.156174\n",
      "619 : 0.190097\n",
      "620 : 0.129429\n",
      "621 : 0.166042\n",
      "622 : 0.142364\n",
      "623 : 0.158819\n",
      "624 : 0.177306\n",
      "625 : 0.191448\n",
      "626 : 0.224269\n",
      "627 : 0.138755\n",
      "628 : 0.165443\n",
      "629 : 0.144171\n",
      "630 : 0.133842\n",
      "631 : 0.181944\n",
      "632 : 0.23608\n",
      "633 : 0.176513\n",
      "634 : 0.124225\n",
      "635 : 0.358699\n",
      "636 : 0.111444\n",
      "637 : 0.14447\n",
      "638 : 0.178302\n",
      "639 : 0.11764\n",
      "640 : 0.252711\n",
      "641 : 0.138954\n",
      "642 : 0.233772\n",
      "643 : 0.168063\n",
      "644 : 0.162546\n",
      "645 : 0.140592\n",
      "646 : 0.238333\n",
      "647 : 0.211661\n",
      "648 : 0.258049\n",
      "649 : 0.208865\n",
      "650 : 0.139671\n",
      "651 : 0.124604\n",
      "652 : 0.10457\n",
      "653 : 0.103351\n",
      "654 : 0.117946\n",
      "655 : 0.203273\n",
      "656 : 0.179064\n",
      "657 : 0.1098\n",
      "658 : 0.146224\n",
      "659 : 0.156824\n",
      "660 : 0.147019\n",
      "661 : 0.194772\n",
      "662 : 0.126046\n",
      "663 : 0.231754\n",
      "664 : 0.111211\n",
      "665 : 0.357515\n",
      "666 : 0.12107\n",
      "667 : 0.153147\n",
      "668 : 0.147692\n",
      "669 : 0.0876952\n",
      "670 : 0.133567\n",
      "671 : 0.184832\n",
      "672 : 0.379135\n",
      "673 : 0.137842\n",
      "674 : 0.14641\n",
      "675 : 0.180221\n",
      "676 : 0.18206\n",
      "677 : 0.148461\n",
      "678 : 0.179227\n",
      "679 : 0.155415\n",
      "680 : 0.238406\n",
      "681 : 0.158704\n",
      "682 : 0.0844103\n",
      "683 : 0.111331\n",
      "684 : 0.194036\n",
      "685 : 0.129346\n",
      "686 : 0.14186\n",
      "687 : 0.161785\n",
      "688 : 0.172297\n",
      "689 : 0.0919321\n",
      "690 : 0.17442\n",
      "691 : 0.137856\n",
      "692 : 0.154908\n",
      "693 : 0.11744\n",
      "694 : 0.133714\n",
      "695 : 0.167778\n",
      "696 : 0.211871\n",
      "697 : 0.170618\n",
      "698 : 0.13285\n",
      "699 : 0.155939\n",
      "700 : 0.115791\n",
      "701 : 0.138965\n",
      "702 : 0.163017\n",
      "703 : 0.109877\n",
      "704 : 0.140707\n",
      "705 : 0.108555\n",
      "706 : 0.143555\n",
      "707 : 0.152361\n",
      "708 : 0.166066\n",
      "709 : 0.163862\n",
      "710 : 0.128878\n",
      "711 : 0.140113\n",
      "712 : 0.120217\n",
      "713 : 0.110355\n",
      "714 : 0.155941\n",
      "715 : 0.160105\n",
      "716 : 0.151621\n",
      "717 : 0.103655\n",
      "718 : 0.241403\n",
      "719 : 0.102527\n",
      "720 : 0.129469\n",
      "721 : 0.154695\n",
      "722 : 0.269964\n",
      "723 : 0.353914\n",
      "724 : 0.187404\n",
      "725 : 0.195373\n",
      "726 : 0.145766\n",
      "727 : 0.126535\n",
      "728 : 0.101773\n",
      "729 : 0.142035\n",
      "730 : 0.192258\n",
      "731 : 0.357393\n",
      "732 : 0.175149\n",
      "733 : 0.127725\n",
      "734 : 0.104788\n",
      "735 : 0.0909057\n",
      "736 : 0.0837961\n",
      "737 : 0.100681\n",
      "738 : 0.162963\n",
      "739 : 0.147142\n",
      "740 : 0.0919216\n",
      "741 : 0.125184\n",
      "742 : 0.132903\n",
      "743 : 0.136121\n",
      "744 : 0.136699\n",
      "745 : 0.103733\n",
      "746 : 0.19894\n",
      "747 : 0.0996304\n",
      "748 : 0.393265\n",
      "749 : 0.0992117\n",
      "750 : 0.128796\n",
      "751 : 0.135383\n",
      "752 : 0.0775411\n",
      "753 : 0.118751\n",
      "754 : 0.162829\n",
      "755 : 0.333794\n",
      "756 : 0.133803\n",
      "757 : 0.12972\n",
      "758 : 0.153734\n",
      "759 : 0.165666\n",
      "760 : 0.138729\n",
      "761 : 0.151197\n",
      "762 : 0.141\n",
      "763 : 0.203728\n",
      "764 : 0.146369\n",
      "765 : 0.0915919\n",
      "766 : 0.100413\n",
      "767 : 0.174104\n",
      "768 : 0.26171\n",
      "769 : 0.130181\n",
      "770 : 0.139015\n",
      "771 : 0.167275\n",
      "772 : 0.081498\n",
      "773 : 0.157967\n",
      "774 : 0.120946\n",
      "775 : 0.137433\n",
      "776 : 0.107893\n",
      "777 : 0.128791\n",
      "778 : 0.147702\n",
      "779 : 0.192129\n",
      "780 : 0.157017\n",
      "781 : 0.124225\n",
      "782 : 0.137102\n",
      "783 : 0.115259\n",
      "784 : 0.132952\n",
      "785 : 0.154748\n",
      "786 : 0.0969148\n",
      "787 : 0.135597\n",
      "788 : 0.102291\n",
      "789 : 0.142467\n",
      "790 : 0.146148\n",
      "791 : 0.133328\n",
      "792 : 0.150703\n",
      "793 : 0.13437\n",
      "794 : 0.113574\n",
      "795 : 0.113857\n",
      "796 : 0.0958415\n",
      "797 : 0.138743\n",
      "798 : 0.202013\n",
      "799 : 0.137706\n",
      "800 : 0.0960508\n",
      "801 : 0.337757\n",
      "802 : 0.08498\n",
      "803 : 0.118015\n",
      "804 : 0.12461\n",
      "805 : 0.0976015\n",
      "806 : 0.197358\n",
      "807 : 0.138719\n",
      "808 : 0.172259\n",
      "809 : 0.128106\n",
      "810 : 0.130452\n",
      "811 : 0.108657\n",
      "812 : 0.187331\n",
      "813 : 0.173447\n",
      "814 : 0.404695\n",
      "815 : 0.158172\n",
      "816 : 0.129834\n",
      "817 : 0.0972475\n",
      "818 : 0.074228\n",
      "819 : 0.0787291\n",
      "820 : 0.131884\n",
      "821 : 0.108954\n",
      "822 : 0.146934\n",
      "823 : 0.0781399\n",
      "824 : 0.104728\n",
      "825 : 0.131666\n",
      "826 : 0.139188\n",
      "827 : 0.135013\n",
      "828 : 0.121797\n",
      "829 : 0.152194\n",
      "830 : 0.0840412\n",
      "831 : 0.38705\n",
      "832 : 0.0934307\n",
      "833 : 0.113856\n",
      "834 : 0.145828\n",
      "835 : 0.067241\n",
      "836 : 0.107025\n",
      "837 : 0.163084\n",
      "838 : 0.277001\n",
      "839 : 0.123517\n",
      "840 : 0.112004\n",
      "841 : 0.124191\n",
      "842 : 0.141668\n",
      "843 : 0.128183\n",
      "844 : 0.118973\n",
      "845 : 0.128615\n",
      "846 : 0.157719\n",
      "847 : 0.125856\n",
      "848 : 0.0806437\n",
      "849 : 0.0913089\n",
      "850 : 0.130355\n",
      "851 : 0.104229\n",
      "852 : 0.106951\n",
      "853 : 0.118299\n",
      "854 : 0.148601\n",
      "855 : 0.0693743\n",
      "856 : 0.142777\n",
      "857 : 0.0993968\n",
      "858 : 0.131896\n",
      "859 : 0.0952347\n",
      "860 : 0.119044\n",
      "861 : 0.131738\n",
      "862 : 0.225485\n",
      "863 : 0.112815\n",
      "864 : 0.117068\n",
      "865 : 0.116595\n",
      "866 : 0.104484\n",
      "867 : 0.120476\n",
      "868 : 0.127292\n",
      "869 : 0.0827665\n",
      "870 : 0.116383\n",
      "871 : 0.0856685\n",
      "872 : 0.157989\n",
      "873 : 0.142738\n",
      "874 : 0.10713\n",
      "875 : 0.140162\n",
      "876 : 0.132135\n",
      "877 : 0.101348\n",
      "878 : 0.104388\n",
      "879 : 0.087824\n",
      "880 : 0.124527\n",
      "881 : 0.130861\n",
      "882 : 0.12315\n",
      "883 : 0.0788851\n",
      "884 : 0.228483\n",
      "885 : 0.0857601\n",
      "886 : 0.0982375\n",
      "887 : 0.151409\n",
      "888 : 0.0846471\n",
      "889 : 0.135093\n",
      "890 : 0.136965\n",
      "891 : 0.16294\n",
      "892 : 0.120718\n",
      "893 : 0.122733\n",
      "894 : 0.0951451\n",
      "895 : 0.179897\n",
      "896 : 0.145506\n",
      "897 : 0.204361\n",
      "898 : 0.143218\n",
      "899 : 0.134405\n",
      "900 : 0.0844303\n",
      "901 : 0.0882815\n",
      "902 : 0.0771506\n",
      "903 : 0.12402\n",
      "904 : 0.0930827\n",
      "905 : 0.139401\n",
      "906 : 0.0808967\n",
      "907 : 0.0964603\n",
      "908 : 0.125077\n",
      "909 : 0.123226\n",
      "910 : 0.139387\n",
      "911 : 0.117859\n",
      "912 : 0.142545\n",
      "913 : 0.0850888\n",
      "914 : 0.246512\n",
      "915 : 0.0827883\n",
      "916 : 0.10711\n",
      "917 : 0.151945\n",
      "918 : 0.0696679\n",
      "919 : 0.104985\n",
      "920 : 0.154646\n",
      "921 : 0.256694\n",
      "922 : 0.120914\n",
      "923 : 0.102685\n",
      "924 : 0.11108\n",
      "925 : 0.137481\n",
      "926 : 0.123741\n",
      "927 : 0.107186\n",
      "928 : 0.125297\n",
      "929 : 0.134632\n",
      "930 : 0.126317\n",
      "931 : 0.105566\n",
      "932 : 0.0997214\n",
      "933 : 0.123087\n",
      "934 : 0.224884\n",
      "935 : 0.101451\n",
      "936 : 0.10667\n",
      "937 : 0.136154\n",
      "938 : 0.0688889\n",
      "939 : 0.134803\n",
      "940 : 0.102883\n",
      "941 : 0.112407\n",
      "942 : 0.0779527\n",
      "943 : 0.121867\n",
      "944 : 0.164635\n",
      "945 : 0.246592\n",
      "946 : 0.116151\n",
      "947 : 0.115627\n",
      "948 : 0.107195\n",
      "949 : 0.121971\n",
      "950 : 0.112637\n",
      "951 : 0.11473\n",
      "952 : 0.0831648\n",
      "953 : 0.102805\n",
      "954 : 0.0909097\n",
      "955 : 0.154444\n",
      "956 : 0.13756\n",
      "957 : 0.0934614\n",
      "958 : 0.16003\n",
      "959 : 0.158724\n",
      "960 : 0.0914054\n",
      "961 : 0.0986664\n",
      "962 : 0.0767299\n",
      "963 : 0.121707\n",
      "964 : 0.136093\n",
      "965 : 0.11706\n",
      "966 : 0.0681481\n",
      "967 : 0.21193\n",
      "968 : 0.0746011\n",
      "969 : 0.0904114\n",
      "970 : 0.124687\n",
      "971 : 0.0753661\n",
      "972 : 0.105651\n",
      "973 : 0.116742\n",
      "974 : 0.137413\n",
      "975 : 0.107398\n",
      "976 : 0.0906846\n",
      "977 : 0.0806525\n",
      "978 : 0.108798\n",
      "979 : 0.127393\n",
      "980 : 0.216138\n",
      "981 : 0.134981\n",
      "982 : 0.0768735\n",
      "983 : 0.0782951\n",
      "984 : 0.0661418\n",
      "985 : 0.0630513\n",
      "986 : 0.0998977\n",
      "987 : 0.0841621\n",
      "988 : 0.111498\n",
      "989 : 0.0685397\n",
      "990 : 0.0821055\n",
      "991 : 0.103356\n",
      "992 : 0.103594\n",
      "993 : 0.105423\n",
      "994 : 0.101308\n",
      "995 : 0.120383\n",
      "996 : 0.0798417\n",
      "997 : 0.220649\n",
      "998 : 0.0719031\n",
      "999 : 0.0905998\n",
      "1000 : 0.0809481\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000\n",
    "preds = []\n",
    "new_input=[]\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    encoded_dataset = combined_dataset[:train_dataset.shape[0], :]\n",
    "    for step in range(1, num_steps + 1):\n",
    "        offset = (step * batch_size) % (encoded_dataset.shape[0] - batch_size)\n",
    "        batch_data = encoded_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = list(train_targets[offset:(offset + batch_size)])\n",
    "        for i, label in enumerate(batch_labels):\n",
    "            batch_labels[i] = (label == np.arange(2)).astype(np.int32)\n",
    "        batch_labels = np.array(batch_labels, dtype=np.int32)\n",
    "        feed_dict = {inputs['l1']: batch_data, targets['l3']: batch_labels}\n",
    "        _, l = session.run([optimizer_l3, loss_l3], feed_dict=feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            print step, ':', l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    logit_op = full_compute()\n",
    "    softmaxes = get_sigmoid(logit_op)\n",
    "    preds = softmaxes.eval(feed_dict={inputs['l1']: combined_dataset[train_dataset.shape[0]:, :]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == labels)\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.94375443577005"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.82571083,  0.30747864],\n",
       "       [ 0.8436923 ,  0.26298583],\n",
       "       [ 0.8436923 ,  0.26298583],\n",
       "       ..., \n",
       "       [ 0.77494293,  0.26730353],\n",
       "       [ 0.8436923 ,  0.26298583],\n",
       "       [ 0.82551992,  0.30795208]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "('Minibatch loss at step', 0, ':', 222.77254)\n",
      "('Minibatch loss at step', 500, ':', 89.292938)\n",
      "('Minibatch loss at step', 1000, ':', 39.540432)\n",
      "('Minibatch loss at step', 1500, ':', 16.709871)\n",
      "('Minibatch loss at step', 2000, ':', 26.412645)\n",
      "('Minibatch loss at step', 2500, ':', 3.530025)\n",
      "('Minibatch loss at step', 3000, ':', 1.5838422)\n",
      "('Minibatch loss at step', 3500, ':', 0.6859898)\n",
      "('Minibatch loss at step', 4000, ':', 0.40181309)\n",
      "('Minibatch loss at step', 4500, ':', 0.38369343)\n",
      "('Minibatch loss at step', 5000, ':', 0.31740069)\n",
      "('Minibatch loss at step', 5500, ':', 0.33525366)\n",
      "('Minibatch loss at step', 6000, ':', 0.36136743)\n",
      "('Minibatch loss at step', 6500, ':', 0.29976884)\n",
      "('Minibatch logloss at step', 0, ':', 2.0832458)\n",
      "('Minibatch logloss at step', 10, ':', 0.43901727)\n",
      "('Minibatch logloss at step', 20, ':', 0.27650648)\n",
      "('Minibatch logloss at step', 30, ':', 0.19391036)\n",
      "('Minibatch logloss at step', 40, ':', 0.22965193)\n",
      "('Minibatch logloss at step', 50, ':', 0.12823243)\n",
      "('Minibatch logloss at step', 60, ':', 0.11483277)\n",
      "('Minibatch logloss at step', 70, ':', 0.11111413)\n",
      "('Minibatch logloss at step', 80, ':', 0.10408607)\n",
      "('Minibatch logloss at step', 90, ':', 0.10029998)\n",
      "('Minibatch logloss at step', 100, ':', 0.086478457)\n",
      "('Minibatch logloss at step', 110, ':', 0.077428266)\n",
      "('Minibatch logloss at step', 120, ':', 0.071148328)\n",
      "('Minibatch logloss at step', 130, ':', 0.09387514)\n",
      "('Minibatch logloss at step', 140, ':', 0.066410065)\n",
      "('Minibatch logloss at step', 150, ':', 0.067981936)\n",
      "('Minibatch logloss at step', 160, ':', 0.062553383)\n",
      "('Minibatch logloss at step', 170, ':', 0.056775603)\n",
      "('Minibatch logloss at step', 180, ':', 0.061193909)\n",
      "('Minibatch logloss at step', 190, ':', 0.071570233)\n",
      "('Minibatch logloss at step', 200, ':', 0.048843376)\n",
      "('Minibatch logloss at step', 210, ':', 0.066191941)\n",
      "('Minibatch logloss at step', 220, ':', 0.047093712)\n",
      "('Minibatch logloss at step', 230, ':', 0.053074576)\n",
      "('Minibatch logloss at step', 240, ':', 0.054819558)\n",
      "('Minibatch logloss at step', 250, ':', 0.045948289)\n",
      "('Minibatch logloss at step', 260, ':', 0.048943795)\n",
      "('Minibatch logloss at step', 270, ':', 0.063280374)\n",
      "('Minibatch logloss at step', 280, ':', 0.05275777)\n",
      "('Minibatch logloss at step', 290, ':', 0.050369803)\n",
      "('Minibatch logloss at step', 300, ':', 0.044953842)\n",
      "('Minibatch logloss at step', 310, ':', 0.044143319)\n",
      "('Minibatch logloss at step', 320, ':', 0.040870674)\n",
      "('Minibatch logloss at step', 330, ':', 0.041033104)\n",
      "('Minibatch logloss at step', 340, ':', 0.044404116)\n",
      "('Minibatch logloss at step', 350, ':', 0.046988081)\n",
      "('Minibatch logloss at step', 360, ':', 0.041888922)\n",
      "0.58601\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7000\n",
    "preds = []\n",
    "randomized_dataset = combined_dataset.copy()\n",
    "#     np.random.shuffle(randomized_dataset)\n",
    "new_input=[]\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (randomized_dataset.shape[0] - batch_size)\n",
    "        batch_data = randomized_dataset[offset:(offset + batch_size), :]\n",
    "        noisy_batch_data = batch_data.copy()\n",
    "        for i, point in enumerate(noisy_batch_data):\n",
    "            noisy_batch_data[i, :] = point + np.random.normal(0, 1, noisy_batch_data.shape[1])\n",
    "        feed_dict = {input_data: noisy_batch_data, clean_data: batch_data}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step\", step, \":\", l)\n",
    "#             print \"SKLEARN loss\", np.sqrt(mean_squared_error(predictions, batch_data))\n",
    "    new_input=layer_1.eval(feed_dict = {input_data :combined_dataset})\n",
    "    test_targets_1hot = np.array([(x == np.arange(2)).astype(np.float32) for x in test_targets])\n",
    "#     print(test_targets_1hot[:5])\n",
    "    for step in range(370):\n",
    "        c = np.random.choice(train_targets.shape[0], 2000, replace=False)\n",
    "        batch_data = train_dataset[:,:-1][c]\n",
    "        batch_targets = train_targets[c]\n",
    "        batch_targets=np.array([(x == np.arange(2)).astype(np.float32) for x in batch_targets])\n",
    "        feed_dict = {clean_data:batch_data, target:batch_targets}\n",
    "        l, _ = session.run([loss_, optimize_], feed_dict=feed_dict)\n",
    "        if (step % 10 == 0):\n",
    "            print(\"Minibatch logloss at step\", step, \":\", l)\n",
    "    new_input = layer_1_f.eval(feed_dict={clean_data:combined_dataset})\n",
    "    print(session.run(a_, feed_dict={clean_data:combined_dataset[len(train_targets):],target:test_targets_1hot}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125973, 123)\n",
      "((125973, 201), (22544, 201))\n"
     ]
    }
   ],
   "source": [
    "#saving new train & test data \n",
    "from collections import Counter\n",
    "print(train_dataset.shape)\n",
    "new_train_data=new_input[:train_dataset.shape[0]]\n",
    "new_train_data=np.c_[new_train_data,train_targets]\n",
    "new_test_data=new_input[train_dataset.shape[0]:]\n",
    "new_test_data=np.c_[new_test_data, test_targets]\n",
    "print(new_train_data.shape, new_test_data.shape)\n",
    "# np.savez('out/new_input.npz',train=new_train_data, test=new_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=1,class_weight='balanced',n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.731867870076\n"
     ]
    }
   ],
   "source": [
    "print (cross_val_score(model, new_train_data[:, :-1], train_targets, cv=3, scoring='precision_macro').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_train_data = np.load('out/new_input.npz')['train']\n",
    "new_test_data = np.load('out/new_input.npz')['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(new_train_data[:, :-1], train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(new_test_data[:, :-1])\n",
    "# np.savez('out/preds.npz',preds=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.740684882896\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(preds, test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7736 5097]\n",
      " [ 749 8962]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_targets, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
