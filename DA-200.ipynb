{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = np.genfromtxt('resources/train.csv', dtype=np.float32, delimiter=',')\n",
    "train_targets = train_dataset[:,-1]\n",
    "train_targets = np.array([1 if x == 2 else 0 for x in train_targets])\n",
    "test_dataset = np.genfromtxt('resources/test.csv', dtype=np.float32, delimiter=',')\n",
    "test_targets = test_dataset[:,-1]\n",
    "test_targets = np.array([1 if x == 2 else 0 for x in test_targets])\n",
    "combined_dataset=np.concatenate((train_dataset[:,:-1], test_dataset[:,:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58630, 67343])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "numeric_indices = []\n",
    "for i in range(combined_dataset.shape[1]):\n",
    "    n_unique = len(np.unique(combined_dataset[:, i]))\n",
    "    numeric_indices.append(n_unique > 2)\n",
    "print (len(numeric_indices))\n",
    "numeric_indices = np.array(numeric_indices, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_dataset[:, numeric_indices] = StandardScaler().fit_transform(\n",
    "                                        combined_dataset[:, numeric_indices]\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 1500\n",
    "num_feature = 122\n",
    "num_labels = 2\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    target = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "    n_input = num_feature\n",
    "    n_hidden_1 = 200\n",
    "    n_hidden_2 = 20\n",
    "    n_hidden_3 = 5\n",
    "    \n",
    "    inputs = {'l1': tf.placeholder(tf.float32, shape=(None, num_feature)),\n",
    "              'l2': tf.placeholder(tf.float32, shape=(None, n_hidden_1)),\n",
    "              'l3': tf.placeholder(tf.float32, shape=(None, n_hidden_2))}\n",
    "    \n",
    "    targets = {'l1': tf.placeholder(tf.float32, shape=(None, num_feature)),\n",
    "               'l2': tf.placeholder(tf.float32, shape=(None, n_hidden_1)),\n",
    "               'l3': tf.placeholder(tf.float32, shape=(None, num_labels))}\n",
    "\n",
    "    weights = {'l1': tf.Variable(tf.truncated_normal([n_input, n_hidden_1], dtype=tf.float32)),\n",
    "               'l2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], dtype=tf.float32)),\n",
    "               'l3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3], dtype=tf.float32))}\n",
    "\n",
    "    enc_biases = {'l1': tf.Variable(tf.truncated_normal([n_hidden_1], dtype=tf.float32)),\n",
    "                  'l2': tf.Variable(tf.truncated_normal([n_hidden_2], dtype=tf.float32)),\n",
    "                  'l3': tf.Variable(tf.truncated_normal([n_hidden_3], dtype=tf.float32))}\n",
    "    \n",
    "    dec_biases = {'l1': tf.Variable(tf.truncated_normal([n_input], dtype=tf.float32)),\n",
    "                  'l2': tf.Variable(tf.truncated_normal([n_hidden_1], dtype=tf.float32))}\n",
    "    \n",
    "    wo = tf.Variable(tf.truncated_normal([n_hidden_3, num_labels], dtype=tf.float32))\n",
    "    bo = tf.Variable(tf.truncated_normal([num_labels], dtype=tf.float32))\n",
    "    \n",
    "    def encode(layer_id):\n",
    "        return tf.add(tf.matmul(inputs[layer_id], weights[layer_id]), enc_biases[layer_id])\n",
    "    \n",
    "    def decode(layer, layer_id):\n",
    "        return tf.add(tf.matmul(layer, tf.transpose(weights[layer_id])), dec_biases[layer_id])\n",
    "    \n",
    "    def get_loss(inp, out):\n",
    "        difference = tf.sub(inp, out) \n",
    "        return tf.sqrt(tf.reduce_mean(tf.square(difference)))   \n",
    "\n",
    "    def encoder_loss(layer_id):\n",
    "        enc = encode(layer_id)\n",
    "        dec = decode(enc, layer_id)\n",
    "        loss = get_loss(targets[layer_id], dec)\n",
    "        return loss\n",
    "    \n",
    "    def compute_logit(layer_id):\n",
    "        logits = tf.nn.tanh(encode(layer_id))\n",
    "        logits = tf.add(tf.matmul(logits, wo), bo)\n",
    "        return logits\n",
    "    \n",
    "    def logit_loss(logits, layer_id):\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, targets[layer_id]))\n",
    "    \n",
    "    def full_compute():\n",
    "        logits = inputs['l1']\n",
    "        for layer_id in ['l1', 'l2', 'l3']:\n",
    "            logits = tf.add(tf.matmul(logits, weights[layer_id]), enc_biases[layer_id])\n",
    "        logits = tf.nn.tanh(logits)\n",
    "        logits = tf.add(tf.matmul(logits, wo), bo)\n",
    "        return logits\n",
    "        \n",
    "    def get_softmax(logits):\n",
    "        return tf.nn.softmax(logits)\n",
    "    \n",
    "    loss_l1 = encoder_loss('l1')\n",
    "    optimizer_l1 = tf.train.AdamOptimizer(0.001).minimize(loss_l1)\n",
    "    loss_l2 = encoder_loss('l2')\n",
    "    optimizer_l2 = tf.train.AdamOptimizer(0.001).minimize(loss_l2)\n",
    "    logits = full_compute()\n",
    "    loss_l3 = logit_loss(logits, 'l3')\n",
    "    optimizer_l3 = tf.train.AdamOptimizer(0.001).minimize(loss_l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "500 : 90.4956\n",
      "1000 : 40.1479\n",
      "1500 : 17.7211\n",
      "2000 : 8.23503\n",
      "2500 : 3.81714\n",
      "3000 : 1.71798\n",
      "3500"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "randomized_dataset = combined_dataset.copy()\n",
    "np.random.shuffle(randomized_dataset)\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    for step in range(1, num_steps + 1):\n",
    "        offset = (step * batch_size) % (randomized_dataset.shape[0] - batch_size)\n",
    "        batch_data = randomized_dataset[offset:(offset + batch_size), :]\n",
    "        noisy_batch_data = batch_data.copy()\n",
    "        for i, point in enumerate(noisy_batch_data):\n",
    "            noisy_batch_data[i, :] = point + np.random.normal(0, 1, noisy_batch_data.shape[1])\n",
    "        feed_dict = {inputs['l1']: noisy_batch_data, targets['l1']: batch_data}\n",
    "        _, l = session.run([optimizer_l1, loss_l1], feed_dict=feed_dict)\n",
    "        if step % 500 == 0:\n",
    "            print step, ':', l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "1000 : 100.616\n",
      "2000 : 39.0461\n",
      "3000 : 23.9815\n",
      "4000 : 15.5616\n",
      "5000 : 11.6594\n",
      "6000 : 9.45537\n",
      "7000 : 8.33887\n",
      "8000 : 7.54305\n",
      "9000 : 6.23905\n",
      "10000 : 5.16105\n",
      "11000 : 4.83644\n",
      "12000 : 4.52867\n",
      "13000 : 8.02212\n",
      "14000 : 5.08431\n",
      "15000 : 4.56395\n",
      "16000 : 4.66459\n",
      "17000 : 3.8691\n",
      "18000 : 4.63158\n",
      "19000 : 3.68647\n",
      "20000 : 3.8327\n",
      "21000 : 2.91987\n",
      "22000 : 4.47652\n",
      "23000 : 2.59041\n",
      "24000 : 2.88201\n",
      "25000 : 3.51531\n",
      "26000 : 4.05447\n",
      "27000 : 2.63748\n",
      "28000 : 3.0164\n",
      "29000 : 2.74419\n",
      "30000 : 2.50407\n",
      "31000 : 2.4288\n",
      "32000 : 2.99447\n",
      "33000 : 2.36293\n",
      "34000 : 2.39094\n",
      "35000 : 2.17911\n",
      "36000 : 3.39051\n",
      "37000 : 2.32618\n",
      "38000 : 9.0268\n",
      "39000 : 2.11285\n",
      "40000 : 1.96725\n",
      "41000 : 1.93506\n",
      "42000 : 1.91593\n",
      "43000 : 1.78285\n",
      "44000 : 2.05383\n",
      "45000 : 1.96982\n"
     ]
    }
   ],
   "source": [
    "num_steps = 45000\n",
    "preds = []\n",
    "randomized_dataset = combined_dataset.copy()\n",
    "np.random.shuffle(randomized_dataset)\n",
    "new_input=[]\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    encoder_op = encode('l1')\n",
    "    encoded_randomized_dataset = encoder_op.eval(feed_dict={inputs['l1']: randomized_dataset})\n",
    "    for step in range(1, num_steps + 1):\n",
    "        offset = (step * batch_size) % (randomized_dataset.shape[0] - batch_size)\n",
    "        batch_data = encoded_randomized_dataset[offset:(offset + batch_size), :]\n",
    "        noisy_batch_data = batch_data.copy()\n",
    "        for i, point in enumerate(noisy_batch_data):\n",
    "            noisy_batch_data[i, :] = point + np.random.normal(0, 1, noisy_batch_data.shape[1])\n",
    "        feed_dict = {inputs['l2']: batch_data, targets['l2']: noisy_batch_data}\n",
    "        _, l = session.run([optimizer_l2, loss_l2], feed_dict=feed_dict)\n",
    "        if step % 1000 == 0:\n",
    "            print step, ':', l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "1 : 2.22388\n",
      "2 : 1.94381\n",
      "3 : 1.79681\n",
      "4 : 1.45612\n",
      "5 : 1.40533\n",
      "6 : 1.40692\n",
      "7 : 1.3666\n",
      "8 : 1.09709\n",
      "9 : 1.01504\n",
      "10 : 1.13868\n",
      "11 : 0.954195\n",
      "12 : 0.917987\n",
      "13 : 0.815127\n",
      "14 : 0.654933\n",
      "15 : 0.734692\n",
      "16 : 0.661279\n",
      "17 : 0.748547\n",
      "18 : 0.588384\n",
      "19 : 0.604396\n",
      "20 : 0.627425\n",
      "21 : 0.646242\n",
      "22 : 0.508418\n",
      "23 : 0.493728\n",
      "24 : 0.552681\n",
      "25 : 0.427362\n",
      "26 : 0.512788\n",
      "27 : 0.400638\n",
      "28 : 0.494675\n",
      "29 : 0.460342\n",
      "30 : 0.482723\n",
      "31 : 0.4151\n",
      "32 : 0.503488\n",
      "33 : 0.435253\n",
      "34 : 0.370975\n",
      "35 : 0.395682\n",
      "36 : 0.431312\n",
      "37 : 0.344237\n",
      "38 : 0.419713\n",
      "39 : 0.360692\n",
      "40 : 0.378864\n",
      "41 : 0.359312\n",
      "42 : 0.34473\n",
      "43 : 0.366466\n",
      "44 : 0.318184\n",
      "45 : 0.355212\n",
      "46 : 0.388735\n",
      "47 : 0.326182\n",
      "48 : 0.353311\n",
      "49 : 0.296068\n",
      "50 : 0.343794\n",
      "51 : 0.333888\n",
      "52 : 0.343215\n",
      "53 : 0.321457\n",
      "54 : 0.323389\n",
      "55 : 0.324607\n",
      "56 : 0.306327\n",
      "57 : 0.309123\n",
      "58 : 0.408385\n",
      "59 : 0.272896\n",
      "60 : 0.31666\n",
      "61 : 0.268835\n",
      "62 : 0.278422\n",
      "63 : 0.312491\n",
      "64 : 0.300583\n",
      "65 : 0.286019\n",
      "66 : 0.277777\n",
      "67 : 0.206374\n",
      "68 : 0.295065\n",
      "69 : 0.289893\n",
      "70 : 0.249844\n",
      "71 : 0.215261\n",
      "72 : 0.254212\n",
      "73 : 0.284041\n",
      "74 : 0.243793\n",
      "75 : 0.27916\n",
      "76 : 0.230856\n",
      "77 : 0.228575\n",
      "78 : 0.252392\n",
      "79 : 0.262422\n",
      "80 : 0.288641\n",
      "81 : 0.168111\n",
      "82 : 0.230693\n",
      "83 : 0.172535\n",
      "84 : 0.221396\n",
      "85 : 0.235268\n",
      "86 : 0.295693\n",
      "87 : 0.216509\n",
      "88 : 0.200064\n",
      "89 : 0.225492\n",
      "90 : 0.247011\n",
      "91 : 0.2597\n",
      "92 : 0.169\n",
      "93 : 0.216084\n",
      "94 : 0.173349\n",
      "95 : 0.254839\n",
      "96 : 0.230167\n",
      "97 : 0.176243\n",
      "98 : 0.175191\n",
      "99 : 0.173314\n",
      "100 : 0.224594\n",
      "101 : 0.19985\n",
      "102 : 0.209081\n",
      "103 : 0.206481\n",
      "104 : 0.244372\n",
      "105 : 0.200513\n",
      "106 : 0.157933\n",
      "107 : 0.212792\n",
      "108 : 0.152315\n",
      "109 : 0.258929\n",
      "110 : 0.16816\n",
      "111 : 0.18951\n",
      "112 : 0.16049\n",
      "113 : 0.20104\n",
      "114 : 0.21298\n",
      "115 : 0.235823\n",
      "116 : 0.195037\n",
      "117 : 0.185935\n",
      "118 : 0.204695\n",
      "119 : 0.235777\n",
      "120 : 0.174077\n",
      "121 : 0.205705\n",
      "122 : 0.199081\n",
      "123 : 0.218797\n",
      "124 : 0.170767\n",
      "125 : 0.157338\n",
      "126 : 0.174571\n",
      "127 : 0.158165\n",
      "128 : 0.174176\n",
      "129 : 0.205411\n",
      "130 : 0.148334\n",
      "131 : 0.157936\n",
      "132 : 0.169197\n",
      "133 : 0.163096\n",
      "134 : 0.169045\n",
      "135 : 0.178079\n",
      "136 : 0.152632\n",
      "137 : 0.167862\n",
      "138 : 0.156777\n",
      "139 : 0.194438\n",
      "140 : 0.132009\n",
      "141 : 0.172086\n",
      "142 : 0.137983\n",
      "143 : 0.150261\n",
      "144 : 0.137857\n",
      "145 : 0.164372\n",
      "146 : 0.171313\n",
      "147 : 0.168033\n",
      "148 : 0.187313\n",
      "149 : 0.145452\n",
      "150 : 0.121472\n",
      "151 : 0.169215\n",
      "152 : 0.167036\n",
      "153 : 0.146209\n",
      "154 : 0.118129\n",
      "155 : 0.164957\n",
      "156 : 0.142293\n",
      "157 : 0.133301\n",
      "158 : 0.159102\n",
      "159 : 0.147849\n",
      "160 : 0.138848\n",
      "161 : 0.166005\n",
      "162 : 0.166452\n",
      "163 : 0.16148\n",
      "164 : 0.125394\n",
      "165 : 0.135607\n",
      "166 : 0.128017\n",
      "167 : 0.139667\n",
      "168 : 0.172559\n",
      "169 : 0.191484\n",
      "170 : 0.132889\n",
      "171 : 0.117972\n",
      "172 : 0.126409\n",
      "173 : 0.159495\n",
      "174 : 0.158714\n",
      "175 : 0.111801\n",
      "176 : 0.112503\n",
      "177 : 0.11682\n",
      "178 : 0.168309\n",
      "179 : 0.152382\n",
      "180 : 0.134322\n",
      "181 : 0.101795\n",
      "182 : 0.114064\n",
      "183 : 0.141127\n",
      "184 : 0.110316\n",
      "185 : 0.142558\n",
      "186 : 0.14547\n",
      "187 : 0.170379\n",
      "188 : 0.159025\n",
      "189 : 0.104953\n",
      "190 : 0.140784\n",
      "191 : 0.118744\n",
      "192 : 0.185843\n",
      "193 : 0.138858\n",
      "194 : 0.135235\n",
      "195 : 0.117473\n",
      "196 : 0.154953\n",
      "197 : 0.143593\n",
      "198 : 0.183907\n",
      "199 : 0.132908\n",
      "200 : 0.115548\n",
      "201 : 0.130245\n",
      "202 : 0.184877\n",
      "203 : 0.0970401\n",
      "204 : 0.148941\n",
      "205 : 0.151138\n",
      "206 : 0.145289\n",
      "207 : 0.110626\n",
      "208 : 0.118526\n",
      "209 : 0.127162\n",
      "210 : 0.132542\n",
      "211 : 0.119746\n",
      "212 : 0.156562\n",
      "213 : 0.115651\n",
      "214 : 0.118918\n",
      "215 : 0.116949\n",
      "216 : 0.120523\n",
      "217 : 0.141193\n",
      "218 : 0.136344\n",
      "219 : 0.100829\n",
      "220 : 0.123487\n",
      "221 : 0.125544\n",
      "222 : 0.117553\n",
      "223 : 0.102951\n",
      "224 : 0.137261\n",
      "225 : 0.0958217\n",
      "226 : 0.125966\n",
      "227 : 0.111611\n",
      "228 : 0.126378\n",
      "229 : 0.134629\n",
      "230 : 0.124176\n",
      "231 : 0.134889\n",
      "232 : 0.127493\n",
      "233 : 0.103913\n",
      "234 : 0.130972\n",
      "235 : 0.133804\n",
      "236 : 0.106151\n",
      "237 : 0.0824243\n",
      "238 : 0.12512\n",
      "239 : 0.107596\n",
      "240 : 0.126483\n",
      "241 : 0.125903\n",
      "242 : 0.126247\n",
      "243 : 0.110898\n",
      "244 : 0.126041\n",
      "245 : 0.108674\n",
      "246 : 0.122905\n",
      "247 : 0.119119\n",
      "248 : 0.0972666\n",
      "249 : 0.0951289\n",
      "250 : 0.122715\n",
      "251 : 0.126591\n",
      "252 : 0.15149\n",
      "253 : 0.114258\n",
      "254 : 0.0973868\n",
      "255 : 0.105838\n",
      "256 : 0.124688\n",
      "257 : 0.120793\n",
      "258 : 0.077374\n",
      "259 : 0.0852012\n",
      "260 : 0.0946529\n",
      "261 : 0.141728\n",
      "262 : 0.115403\n",
      "263 : 0.100955\n",
      "264 : 0.08454\n",
      "265 : 0.109125\n",
      "266 : 0.11139\n",
      "267 : 0.0971451\n",
      "268 : 0.114354\n",
      "269 : 0.117375\n",
      "270 : 0.129351\n",
      "271 : 0.106777\n",
      "272 : 0.0775962\n",
      "273 : 0.149935\n",
      "274 : 0.0920081\n",
      "275 : 0.142364\n",
      "276 : 0.105815\n",
      "277 : 0.108646\n",
      "278 : 0.103396\n",
      "279 : 0.107808\n",
      "280 : 0.121406\n",
      "281 : 0.135352\n",
      "282 : 0.115704\n",
      "283 : 0.0911778\n",
      "284 : 0.109116\n",
      "285 : 0.141775\n",
      "286 : 0.0681108\n",
      "287 : 0.132519\n",
      "288 : 0.112923\n",
      "289 : 0.124047\n",
      "290 : 0.0891435\n",
      "291 : 0.100405\n",
      "292 : 0.118382\n",
      "293 : 0.10288\n",
      "294 : 0.101371\n",
      "295 : 0.142129\n",
      "296 : 0.101497\n",
      "297 : 0.0907157\n",
      "298 : 0.0918049\n",
      "299 : 0.106802\n",
      "300 : 0.125435\n",
      "301 : 0.116856\n",
      "302 : 0.0933146\n",
      "303 : 0.0991409\n",
      "304 : 0.119216\n",
      "305 : 0.111519\n",
      "306 : 0.0702641\n",
      "307 : 0.10935\n",
      "308 : 0.082966\n",
      "309 : 0.110324\n",
      "310 : 0.102671\n",
      "311 : 0.104231\n",
      "312 : 0.108429\n",
      "313 : 0.101494\n",
      "314 : 0.106614\n",
      "315 : 0.0876863\n",
      "316 : 0.0885494\n",
      "317 : 0.0963273\n",
      "318 : 0.113367\n",
      "319 : 0.0871939\n",
      "320 : 0.0654501\n",
      "321 : 0.105211\n",
      "322 : 0.0917896\n",
      "323 : 0.0981308\n",
      "324 : 0.118813\n",
      "325 : 0.0991502\n",
      "326 : 0.0998378\n",
      "327 : 0.103636\n",
      "328 : 0.0934201\n",
      "329 : 0.129405\n",
      "330 : 0.0938032\n",
      "331 : 0.0880725\n",
      "332 : 0.078283\n",
      "333 : 0.0974884\n",
      "334 : 0.0979096\n",
      "335 : 0.125386\n",
      "336 : 0.0966071\n",
      "337 : 0.0831197\n",
      "338 : 0.0971527\n",
      "339 : 0.109621\n",
      "340 : 0.11067\n",
      "341 : 0.073119\n",
      "342 : 0.0838258\n",
      "343 : 0.103243\n",
      "344 : 0.126461\n",
      "345 : 0.101391\n",
      "346 : 0.0943768\n",
      "347 : 0.0649575\n",
      "348 : 0.106171\n",
      "349 : 0.101757\n",
      "350 : 0.0849163\n",
      "351 : 0.0994589\n",
      "352 : 0.0951166\n",
      "353 : 0.119756\n",
      "354 : 0.0922754\n",
      "355 : 0.0645329\n",
      "356 : 0.11997\n",
      "357 : 0.0778583\n",
      "358 : 0.134686\n",
      "359 : 0.0871395\n",
      "360 : 0.102542\n",
      "361 : 0.0895685\n",
      "362 : 0.11038\n",
      "363 : 0.108987\n",
      "364 : 0.120637\n",
      "365 : 0.0986418\n",
      "366 : 0.0890763\n",
      "367 : 0.0912606\n",
      "368 : 0.120316\n",
      "369 : 0.0607913\n",
      "370 : 0.108749\n",
      "371 : 0.0962818\n",
      "372 : 0.126416\n",
      "373 : 0.096811\n",
      "374 : 0.07641\n",
      "375 : 0.102936\n",
      "376 : 0.0959632\n",
      "377 : 0.0979926\n",
      "378 : 0.12691\n",
      "379 : 0.0966717\n",
      "380 : 0.08564\n",
      "381 : 0.0758255\n",
      "382 : 0.0913899\n",
      "383 : 0.127461\n",
      "384 : 0.103131\n",
      "385 : 0.0965891\n",
      "386 : 0.0849304\n",
      "387 : 0.103846\n",
      "388 : 0.0972535\n",
      "389 : 0.0688178\n",
      "390 : 0.110519\n",
      "391 : 0.0874663\n",
      "392 : 0.0962527\n",
      "393 : 0.104131\n",
      "394 : 0.106228\n",
      "395 : 0.121457\n",
      "396 : 0.0953233\n",
      "397 : 0.107548\n",
      "398 : 0.0808607\n",
      "399 : 0.0972932\n",
      "400 : 0.111152\n",
      "401 : 0.0997532\n",
      "402 : 0.0721932\n",
      "403 : 0.0704918\n",
      "404 : 0.117518\n",
      "405 : 0.103217\n",
      "406 : 0.117225\n",
      "407 : 0.112989\n",
      "408 : 0.097438\n",
      "409 : 0.0877448\n",
      "410 : 0.114966\n",
      "411 : 0.106356\n",
      "412 : 0.146716\n",
      "413 : 0.0949406\n",
      "414 : 0.102086\n",
      "415 : 0.0975298\n",
      "416 : 0.106298\n",
      "417 : 0.0859509\n",
      "418 : 0.129657\n",
      "419 : 0.106906\n",
      "420 : 0.079404\n",
      "421 : 0.0753176\n",
      "422 : 0.10174\n",
      "423 : 0.116449\n",
      "424 : 0.0723464\n",
      "425 : 0.0726034\n",
      "426 : 0.0778246\n",
      "427 : 0.101666\n",
      "428 : 0.0968866\n",
      "429 : 0.0968543\n",
      "430 : 0.0691343\n",
      "431 : 0.101209\n",
      "432 : 0.0775788\n",
      "433 : 0.078593\n",
      "434 : 0.0889044\n",
      "435 : 0.0929942\n",
      "436 : 0.105828\n",
      "437 : 0.0920228\n",
      "438 : 0.0891387\n",
      "439 : 0.123294\n",
      "440 : 0.0672289\n",
      "441 : 0.110325\n",
      "442 : 0.0884315\n",
      "443 : 0.0926301\n",
      "444 : 0.0908565\n",
      "445 : 0.0916298\n",
      "446 : 0.0858042\n",
      "447 : 0.121874\n",
      "448 : 0.0882963\n",
      "449 : 0.0755206\n",
      "450 : 0.107164\n",
      "451 : 0.106193\n",
      "452 : 0.0624537\n",
      "453 : 0.0933348\n",
      "454 : 0.0818986\n",
      "455 : 0.0967598\n",
      "456 : 0.0878021\n",
      "457 : 0.072654\n",
      "458 : 0.0976982\n",
      "459 : 0.0718275\n",
      "460 : 0.0796739\n",
      "461 : 0.104141\n",
      "462 : 0.0778266\n",
      "463 : 0.0794509\n",
      "464 : 0.0856443\n",
      "465 : 0.0824697\n",
      "466 : 0.10198\n",
      "467 : 0.0824078\n",
      "468 : 0.0742143\n",
      "469 : 0.0786253\n",
      "470 : 0.0771469\n",
      "471 : 0.0840546\n",
      "472 : 0.0537756\n",
      "473 : 0.089323\n",
      "474 : 0.0692463\n",
      "475 : 0.0677867\n",
      "476 : 0.0922922\n",
      "477 : 0.0954174\n",
      "478 : 0.108257\n",
      "479 : 0.0784249\n",
      "480 : 0.0872572\n",
      "481 : 0.0740129\n",
      "482 : 0.0829168\n",
      "483 : 0.0761597\n",
      "484 : 0.0787325\n",
      "485 : 0.0677408\n",
      "486 : 0.0528533\n",
      "487 : 0.109399\n",
      "488 : 0.0721023\n",
      "489 : 0.0996988\n",
      "490 : 0.0869651\n",
      "491 : 0.0836085\n",
      "492 : 0.074434\n",
      "493 : 0.081316\n",
      "494 : 0.06801\n",
      "495 : 0.118928\n",
      "496 : 0.0721933\n",
      "497 : 0.0779449\n",
      "498 : 0.0651115\n",
      "499 : 0.0831984\n",
      "500 : 0.0652916\n",
      "501 : 0.093841\n",
      "502 : 0.0814343\n",
      "503 : 0.0606242\n",
      "504 : 0.0717342\n",
      "505 : 0.0836707\n",
      "506 : 0.0979434\n",
      "507 : 0.0641701\n",
      "508 : 0.0674094\n",
      "509 : 0.0654001\n",
      "510 : 0.0886424\n",
      "511 : 0.0927513\n",
      "512 : 0.0701555\n",
      "513 : 0.0575843\n",
      "514 : 0.0840764\n",
      "515 : 0.0644048\n",
      "516 : 0.0528437\n",
      "517 : 0.0741986\n",
      "518 : 0.0879041\n",
      "519 : 0.0915561\n",
      "520 : 0.0722485\n",
      "521 : 0.0624045\n",
      "522 : 0.0988151\n",
      "523 : 0.0539711\n",
      "524 : 0.0973843\n",
      "525 : 0.0690642\n",
      "526 : 0.0800562\n",
      "527 : 0.0803574\n",
      "528 : 0.0708683\n",
      "529 : 0.0737135\n",
      "530 : 0.100787\n",
      "531 : 0.0732308\n",
      "532 : 0.0718504\n",
      "533 : 0.0892984\n",
      "534 : 0.0815166\n",
      "535 : 0.0534376\n",
      "536 : 0.0847356\n",
      "537 : 0.0851539\n",
      "538 : 0.0791791\n",
      "539 : 0.0724656\n",
      "540 : 0.0618727\n",
      "541 : 0.0857599\n",
      "542 : 0.0689739\n",
      "543 : 0.0756872\n",
      "544 : 0.0836417\n",
      "545 : 0.0775139\n",
      "546 : 0.0676794\n",
      "547 : 0.0649121\n",
      "548 : 0.0663085\n",
      "549 : 0.100019\n",
      "550 : 0.0790037\n",
      "551 : 0.0810377\n",
      "552 : 0.0680698\n",
      "553 : 0.0801003\n",
      "554 : 0.0700445\n",
      "555 : 0.0505145\n",
      "556 : 0.0826962\n",
      "557 : 0.0684271\n",
      "558 : 0.0545595\n",
      "559 : 0.0691586\n",
      "560 : 0.0842009\n",
      "561 : 0.0866954\n",
      "562 : 0.059192\n",
      "563 : 0.0810144\n",
      "564 : 0.0671146\n",
      "565 : 0.0726508\n",
      "566 : 0.0647083\n",
      "567 : 0.0676275\n",
      "568 : 0.0539771\n",
      "569 : 0.0439633\n",
      "570 : 0.0757831\n",
      "571 : 0.0547295\n",
      "572 : 0.0713864\n",
      "573 : 0.0757585\n",
      "574 : 0.0594649\n",
      "575 : 0.0693041\n",
      "576 : 0.0777876\n",
      "577 : 0.063994\n",
      "578 : 0.105593\n",
      "579 : 0.067679\n",
      "580 : 0.0647666\n",
      "581 : 0.0552836\n",
      "582 : 0.0774572\n",
      "583 : 0.0622083\n",
      "584 : 0.0748672\n",
      "585 : 0.086689\n",
      "586 : 0.0542341\n",
      "587 : 0.0745104\n",
      "588 : 0.0733029\n",
      "589 : 0.0891014\n",
      "590 : 0.0563196\n",
      "591 : 0.0533575\n",
      "592 : 0.0560187\n",
      "593 : 0.0878242\n",
      "594 : 0.0848443\n",
      "595 : 0.0568555\n",
      "596 : 0.0560378\n",
      "597 : 0.0910649\n",
      "598 : 0.050038\n",
      "599 : 0.0508523\n",
      "600 : 0.0678992\n",
      "601 : 0.076869\n",
      "602 : 0.0954484\n",
      "603 : 0.0686254\n",
      "604 : 0.0623524\n",
      "605 : 0.0935529\n",
      "606 : 0.057183\n",
      "607 : 0.0893251\n",
      "608 : 0.0711076\n",
      "609 : 0.076545\n",
      "610 : 0.0692069\n",
      "611 : 0.0707973\n",
      "612 : 0.069409\n",
      "613 : 0.0888363\n",
      "614 : 0.0683726\n",
      "615 : 0.0676104\n",
      "616 : 0.0759379\n",
      "617 : 0.0758265\n",
      "618 : 0.0517059\n",
      "619 : 0.075041\n",
      "620 : 0.0757582\n",
      "621 : 0.0815779\n",
      "622 : 0.0702437\n",
      "623 : 0.0550561\n",
      "624 : 0.0801949\n",
      "625 : 0.0756816\n",
      "626 : 0.0725518\n",
      "627 : 0.068334\n",
      "628 : 0.0787794\n",
      "629 : 0.0703507\n",
      "630 : 0.0624413\n",
      "631 : 0.0576745\n",
      "632 : 0.0944025\n",
      "633 : 0.0721466\n",
      "634 : 0.0643139\n",
      "635 : 0.0537239\n",
      "636 : 0.0629637\n",
      "637 : 0.0558654\n",
      "638 : 0.04359\n",
      "639 : 0.0749096\n",
      "640 : 0.075938\n",
      "641 : 0.0531236\n",
      "642 : 0.0719115\n",
      "643 : 0.06949\n",
      "644 : 0.0818054\n",
      "645 : 0.0573425\n",
      "646 : 0.0658059\n",
      "647 : 0.0604448\n",
      "648 : 0.0666133\n",
      "649 : 0.0578008\n",
      "650 : 0.059898\n",
      "651 : 0.0495777\n",
      "652 : 0.0380199\n",
      "653 : 0.0712885\n",
      "654 : 0.0614214\n",
      "655 : 0.0681655\n",
      "656 : 0.0817892\n",
      "657 : 0.0735682\n",
      "658 : 0.0759644\n",
      "659 : 0.0747367\n",
      "660 : 0.0553254\n",
      "661 : 0.0886451\n",
      "662 : 0.0611957\n",
      "663 : 0.0594264\n",
      "664 : 0.0526419\n",
      "665 : 0.0735878\n",
      "666 : 0.0608769\n",
      "667 : 0.0788296\n",
      "668 : 0.0780749\n",
      "669 : 0.0504192\n",
      "670 : 0.0701179\n",
      "671 : 0.0686419\n",
      "672 : 0.081908\n",
      "673 : 0.0597493\n",
      "674 : 0.0519318\n",
      "675 : 0.0600369\n",
      "676 : 0.0809651\n",
      "677 : 0.0713591\n",
      "678 : 0.0590351\n",
      "679 : 0.0561389\n",
      "680 : 0.0748028\n",
      "681 : 0.0566904\n",
      "682 : 0.0451866\n",
      "683 : 0.0565003\n",
      "684 : 0.0748929\n",
      "685 : 0.0703426\n",
      "686 : 0.0581802\n",
      "687 : 0.0593992\n",
      "688 : 0.0876729\n",
      "689 : 0.041996\n",
      "690 : 0.0829918\n",
      "691 : 0.0697154\n",
      "692 : 0.0674458\n",
      "693 : 0.0545812\n",
      "694 : 0.0682816\n",
      "695 : 0.0661872\n",
      "696 : 0.0749915\n",
      "697 : 0.0618983\n",
      "698 : 0.0578499\n",
      "699 : 0.0621223\n",
      "700 : 0.0759073\n",
      "701 : 0.0411009\n",
      "702 : 0.0668775\n",
      "703 : 0.0663412\n",
      "704 : 0.0636203\n",
      "705 : 0.0629407\n",
      "706 : 0.0523098\n",
      "707 : 0.0740552\n",
      "708 : 0.0768927\n",
      "709 : 0.0625302\n",
      "710 : 0.0594341\n",
      "711 : 0.0706311\n",
      "712 : 0.0653419\n",
      "713 : 0.0530741\n",
      "714 : 0.0590653\n",
      "715 : 0.0793926\n",
      "716 : 0.0551271\n",
      "717 : 0.0598023\n",
      "718 : 0.0547625\n",
      "719 : 0.0522303\n",
      "720 : 0.0537119\n",
      "721 : 0.0544592\n",
      "722 : 0.0688658\n",
      "723 : 0.10459\n",
      "724 : 0.0477677\n",
      "725 : 0.0705371\n",
      "726 : 0.0623058\n",
      "727 : 0.0766936\n",
      "728 : 0.0624681\n",
      "729 : 0.0704233\n",
      "730 : 0.0625366\n",
      "731 : 0.0676433\n",
      "732 : 0.0509926\n",
      "733 : 0.0531907\n",
      "734 : 0.0550245\n",
      "735 : 0.0336075\n",
      "736 : 0.0693193\n",
      "737 : 0.0691265\n",
      "738 : 0.072913\n",
      "739 : 0.0727241\n",
      "740 : 0.0651565\n",
      "741 : 0.0625454\n",
      "742 : 0.0735539\n",
      "743 : 0.0506027\n",
      "744 : 0.081801\n",
      "745 : 0.0636271\n",
      "746 : 0.0557044\n",
      "747 : 0.0495686\n",
      "748 : 0.0580558\n",
      "749 : 0.0564181\n",
      "750 : 0.0662655\n",
      "751 : 0.0675517\n",
      "752 : 0.0533339\n",
      "753 : 0.0678732\n",
      "754 : 0.0614682\n",
      "755 : 0.0660979\n",
      "756 : 0.0638225\n",
      "757 : 0.0461855\n",
      "758 : 0.050073\n",
      "759 : 0.0724544\n",
      "760 : 0.0741447\n",
      "761 : 0.0538782\n",
      "762 : 0.074715\n",
      "763 : 0.0782844\n",
      "764 : 0.0390906\n",
      "765 : 0.052361\n",
      "766 : 0.060811\n",
      "767 : 0.071436\n",
      "768 : 0.0683422\n",
      "769 : 0.0588761\n",
      "770 : 0.0526294\n",
      "771 : 0.0879854\n",
      "772 : 0.0390068\n",
      "773 : 0.0801496\n",
      "774 : 0.0736133\n",
      "775 : 0.0649655\n",
      "776 : 0.0429881\n",
      "777 : 0.0722003\n",
      "778 : 0.0552502\n",
      "779 : 0.0915911\n",
      "780 : 0.0628557\n",
      "781 : 0.0527507\n",
      "782 : 0.0648301\n",
      "783 : 0.0590812\n",
      "784 : 0.0389272\n",
      "785 : 0.072709\n",
      "786 : 0.0680564\n",
      "787 : 0.0663915\n",
      "788 : 0.0688348\n",
      "789 : 0.0506779\n",
      "790 : 0.058112\n",
      "791 : 0.0654521\n",
      "792 : 0.0532391\n",
      "793 : 0.058068\n",
      "794 : 0.0787887\n",
      "795 : 0.0544784\n",
      "796 : 0.0593328\n",
      "797 : 0.0652575\n",
      "798 : 0.0745397\n",
      "799 : 0.0521803\n",
      "800 : 0.0471004\n",
      "801 : 0.0503603\n",
      "802 : 0.0518598\n",
      "803 : 0.0628094\n",
      "804 : 0.047735\n",
      "805 : 0.0655804\n",
      "806 : 0.0969809\n",
      "807 : 0.0430536\n",
      "808 : 0.0631275\n",
      "809 : 0.0533077\n",
      "810 : 0.0719897\n",
      "811 : 0.051998\n",
      "812 : 0.0554621\n",
      "813 : 0.0579478\n",
      "814 : 0.0635779\n",
      "815 : 0.0486457\n",
      "816 : 0.0554224\n",
      "817 : 0.0552173\n",
      "818 : 0.0413632\n",
      "819 : 0.0562917\n",
      "820 : 0.0692868\n",
      "821 : 0.0580119\n",
      "822 : 0.0674276\n",
      "823 : 0.0720285\n",
      "824 : 0.0675983\n",
      "825 : 0.0608781\n",
      "826 : 0.0444769\n",
      "827 : 0.0833586\n",
      "828 : 0.0694704\n",
      "829 : 0.0537299\n",
      "830 : 0.0426969\n",
      "831 : 0.0507918\n",
      "832 : 0.055495\n",
      "833 : 0.0654366\n",
      "834 : 0.070792\n",
      "835 : 0.0478808\n",
      "836 : 0.0606281\n",
      "837 : 0.0679461\n",
      "838 : 0.0764642\n",
      "839 : 0.060507\n",
      "840 : 0.0555462\n",
      "841 : 0.0441845\n",
      "842 : 0.063189\n",
      "843 : 0.0579945\n",
      "844 : 0.0522429\n",
      "845 : 0.0653107\n",
      "846 : 0.0743565\n",
      "847 : 0.043584\n",
      "848 : 0.0422409\n",
      "849 : 0.0619721\n",
      "850 : 0.069627\n",
      "851 : 0.0811976\n",
      "852 : 0.0690054\n",
      "853 : 0.0557936\n",
      "854 : 0.0706054\n",
      "855 : 0.0419965\n",
      "856 : 0.0824794\n",
      "857 : 0.0587541\n",
      "858 : 0.0748588\n",
      "859 : 0.0515712\n",
      "860 : 0.0685722\n",
      "861 : 0.0519774\n",
      "862 : 0.0747964\n",
      "863 : 0.0767344\n",
      "864 : 0.0582393\n",
      "865 : 0.0616289\n",
      "866 : 0.0708311\n",
      "867 : 0.0442418\n",
      "868 : 0.0834289\n",
      "869 : 0.0759612\n",
      "870 : 0.0809722\n",
      "871 : 0.0715396\n",
      "872 : 0.058506\n",
      "873 : 0.0630458\n",
      "874 : 0.0747987\n",
      "875 : 0.0638984\n",
      "876 : 0.0534763\n",
      "877 : 0.065733\n",
      "878 : 0.0581257\n",
      "879 : 0.0476802\n",
      "880 : 0.0581067\n",
      "881 : 0.0797694\n",
      "882 : 0.0524671\n",
      "883 : 0.0680353\n",
      "884 : 0.0568168\n",
      "885 : 0.0457724\n",
      "886 : 0.0576347\n",
      "887 : 0.0458722\n",
      "888 : 0.0628119\n",
      "889 : 0.0735978\n",
      "890 : 0.0572748\n",
      "891 : 0.0571777\n",
      "892 : 0.0565473\n",
      "893 : 0.0630586\n",
      "894 : 0.0601189\n",
      "895 : 0.0656909\n",
      "896 : 0.0642696\n",
      "897 : 0.0618277\n",
      "898 : 0.036083\n",
      "899 : 0.0460098\n",
      "900 : 0.0457896\n",
      "901 : 0.0392423\n",
      "902 : 0.0556016\n",
      "903 : 0.0589377\n",
      "904 : 0.0490939\n",
      "905 : 0.0814211\n",
      "906 : 0.0560719\n",
      "907 : 0.0662577\n",
      "908 : 0.0701862\n",
      "909 : 0.041363\n",
      "910 : 0.0850938\n",
      "911 : 0.0515456\n",
      "912 : 0.0571739\n",
      "913 : 0.0403611\n",
      "914 : 0.0631393\n",
      "915 : 0.0516189\n",
      "916 : 0.0593133\n",
      "917 : 0.0602264\n",
      "918 : 0.0390436\n",
      "919 : 0.0616896\n",
      "920 : 0.0631388\n",
      "921 : 0.0736882\n",
      "922 : 0.0571251\n",
      "923 : 0.0517575\n",
      "924 : 0.0444865\n",
      "925 : 0.0629662\n",
      "926 : 0.0551831\n",
      "927 : 0.0540721\n",
      "928 : 0.0644291\n",
      "929 : 0.0728835\n",
      "930 : 0.0407655\n",
      "931 : 0.0365241\n",
      "932 : 0.0478067\n",
      "933 : 0.0637485\n",
      "934 : 0.0671042\n",
      "935 : 0.0614016\n",
      "936 : 0.0491876\n",
      "937 : 0.0851314\n",
      "938 : 0.035955\n",
      "939 : 0.0691322\n",
      "940 : 0.0515561\n",
      "941 : 0.0502481\n",
      "942 : 0.0442569\n",
      "943 : 0.0661128\n",
      "944 : 0.0464957\n",
      "945 : 0.0619449\n",
      "946 : 0.057424\n",
      "947 : 0.0494993\n",
      "948 : 0.0554804\n",
      "949 : 0.0630164\n",
      "950 : 0.0269414\n",
      "951 : 0.0684339\n",
      "952 : 0.0586041\n",
      "953 : 0.0538379\n",
      "954 : 0.059919\n",
      "955 : 0.0437569\n",
      "956 : 0.0459573\n",
      "957 : 0.0619452\n",
      "958 : 0.0437635\n",
      "959 : 0.0532133\n",
      "960 : 0.0531596\n",
      "961 : 0.0473548\n",
      "962 : 0.0525859\n",
      "963 : 0.0426378\n",
      "964 : 0.0702466\n",
      "965 : 0.0480008\n",
      "966 : 0.0462634\n",
      "967 : 0.0423019\n",
      "968 : 0.0496226\n",
      "969 : 0.0433998\n",
      "970 : 0.0461946\n",
      "971 : 0.0529214\n",
      "972 : 0.0738695\n",
      "973 : 0.0388003\n",
      "974 : 0.0510753\n",
      "975 : 0.045815\n",
      "976 : 0.0594287\n",
      "977 : 0.0667605\n",
      "978 : 0.0604929\n",
      "979 : 0.0515857\n",
      "980 : 0.0591849\n",
      "981 : 0.0372791\n",
      "982 : 0.0330905\n",
      "983 : 0.0547129\n",
      "984 : 0.0437554\n",
      "985 : 0.0338189\n",
      "986 : 0.0497727\n",
      "987 : 0.0423213\n",
      "988 : 0.071104\n",
      "989 : 0.0621761\n",
      "990 : 0.0601819\n",
      "991 : 0.0488949\n",
      "992 : 0.0400947\n",
      "993 : 0.0670449\n",
      "994 : 0.0476183\n",
      "995 : 0.0476386\n",
      "996 : 0.0385387\n",
      "997 : 0.0533567\n",
      "998 : 0.0393056\n",
      "999 : 0.0514483\n",
      "1000 : 0.0640874\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000\n",
    "preds = []\n",
    "new_input=[]\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "#     encoder_op = encode('l1')\n",
    "#     encoded_dataset = encoder_op.eval(feed_dict={inputs['l1']: combined_dataset[:train_dataset.shape[0], :]})\n",
    "#     encoder_op = encode('l2')\n",
    "#     encoded_dataset = encoder_op.eval(feed_dict={inputs['l2']: encoded_dataset})\n",
    "    encoded_dataset = combined_dataset[:train_dataset.shape[0], :]\n",
    "    for step in range(1, num_steps + 1):\n",
    "        offset = (step * batch_size) % (encoded_dataset.shape[0] - batch_size)\n",
    "        batch_data = encoded_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = list(train_targets[offset:(offset + batch_size)])\n",
    "        for i, label in enumerate(batch_labels):\n",
    "            batch_labels[i] = (label == np.arange(2)).astype(np.int32)\n",
    "        batch_labels = np.array(batch_labels, dtype=np.int32)\n",
    "        feed_dict = {inputs['l1']: batch_data, targets['l3']: batch_labels}\n",
    "        _, l = session.run([optimizer_l3, loss_l3], feed_dict=feed_dict)\n",
    "#         if step % 100 == 0:\n",
    "        print step, ':', l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "#     encoder_op = encode('l1')\n",
    "#     encoded_dataset = encoder_op.eval(feed_dict={inputs['l1']: combined_dataset[train_dataset.shape[0]:, :]})\n",
    "#     encoder_op = encode('l2')\n",
    "#     encoded_dataset = encoder_op.eval(feed_dict={inputs['l2']: encoded_dataset})\n",
    "    logit_op = full_compute()\n",
    "    softmaxes = get_softmax(logit_op)\n",
    "    preds = softmaxes.eval(feed_dict={inputs['l1']: combined_dataset[train_dataset.shape[0]:, :]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == labels)\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.466997870830376"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "('Minibatch loss at step', 0, ':', 222.77254)\n",
      "('Minibatch loss at step', 500, ':', 89.292938)\n",
      "('Minibatch loss at step', 1000, ':', 39.540432)\n",
      "('Minibatch loss at step', 1500, ':', 16.709871)\n",
      "('Minibatch loss at step', 2000, ':', 26.412645)\n",
      "('Minibatch loss at step', 2500, ':', 3.530025)\n",
      "('Minibatch loss at step', 3000, ':', 1.5838422)\n",
      "('Minibatch loss at step', 3500, ':', 0.6859898)\n",
      "('Minibatch loss at step', 4000, ':', 0.40181309)\n",
      "('Minibatch loss at step', 4500, ':', 0.38369343)\n",
      "('Minibatch loss at step', 5000, ':', 0.31740069)\n",
      "('Minibatch loss at step', 5500, ':', 0.33525366)\n",
      "('Minibatch loss at step', 6000, ':', 0.36136743)\n",
      "('Minibatch loss at step', 6500, ':', 0.29976884)\n",
      "('Minibatch logloss at step', 0, ':', 2.0832458)\n",
      "('Minibatch logloss at step', 10, ':', 0.43901727)\n",
      "('Minibatch logloss at step', 20, ':', 0.27650648)\n",
      "('Minibatch logloss at step', 30, ':', 0.19391036)\n",
      "('Minibatch logloss at step', 40, ':', 0.22965193)\n",
      "('Minibatch logloss at step', 50, ':', 0.12823243)\n",
      "('Minibatch logloss at step', 60, ':', 0.11483277)\n",
      "('Minibatch logloss at step', 70, ':', 0.11111413)\n",
      "('Minibatch logloss at step', 80, ':', 0.10408607)\n",
      "('Minibatch logloss at step', 90, ':', 0.10029998)\n",
      "('Minibatch logloss at step', 100, ':', 0.086478457)\n",
      "('Minibatch logloss at step', 110, ':', 0.077428266)\n",
      "('Minibatch logloss at step', 120, ':', 0.071148328)\n",
      "('Minibatch logloss at step', 130, ':', 0.09387514)\n",
      "('Minibatch logloss at step', 140, ':', 0.066410065)\n",
      "('Minibatch logloss at step', 150, ':', 0.067981936)\n",
      "('Minibatch logloss at step', 160, ':', 0.062553383)\n",
      "('Minibatch logloss at step', 170, ':', 0.056775603)\n",
      "('Minibatch logloss at step', 180, ':', 0.061193909)\n",
      "('Minibatch logloss at step', 190, ':', 0.071570233)\n",
      "('Minibatch logloss at step', 200, ':', 0.048843376)\n",
      "('Minibatch logloss at step', 210, ':', 0.066191941)\n",
      "('Minibatch logloss at step', 220, ':', 0.047093712)\n",
      "('Minibatch logloss at step', 230, ':', 0.053074576)\n",
      "('Minibatch logloss at step', 240, ':', 0.054819558)\n",
      "('Minibatch logloss at step', 250, ':', 0.045948289)\n",
      "('Minibatch logloss at step', 260, ':', 0.048943795)\n",
      "('Minibatch logloss at step', 270, ':', 0.063280374)\n",
      "('Minibatch logloss at step', 280, ':', 0.05275777)\n",
      "('Minibatch logloss at step', 290, ':', 0.050369803)\n",
      "('Minibatch logloss at step', 300, ':', 0.044953842)\n",
      "('Minibatch logloss at step', 310, ':', 0.044143319)\n",
      "('Minibatch logloss at step', 320, ':', 0.040870674)\n",
      "('Minibatch logloss at step', 330, ':', 0.041033104)\n",
      "('Minibatch logloss at step', 340, ':', 0.044404116)\n",
      "('Minibatch logloss at step', 350, ':', 0.046988081)\n",
      "('Minibatch logloss at step', 360, ':', 0.041888922)\n",
      "0.58601\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7000\n",
    "preds = []\n",
    "randomized_dataset = combined_dataset.copy()\n",
    "#     np.random.shuffle(randomized_dataset)\n",
    "new_input=[]\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (randomized_dataset.shape[0] - batch_size)\n",
    "        batch_data = randomized_dataset[offset:(offset + batch_size), :]\n",
    "        noisy_batch_data = batch_data.copy()\n",
    "        for i, point in enumerate(noisy_batch_data):\n",
    "            noisy_batch_data[i, :] = point + np.random.normal(0, 1, noisy_batch_data.shape[1])\n",
    "        feed_dict = {input_data: noisy_batch_data, clean_data: batch_data}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step\", step, \":\", l)\n",
    "#             print \"SKLEARN loss\", np.sqrt(mean_squared_error(predictions, batch_data))\n",
    "    new_input=layer_1.eval(feed_dict = {input_data :combined_dataset})\n",
    "    test_targets_1hot = np.array([(x == np.arange(2)).astype(np.float32) for x in test_targets])\n",
    "#     print(test_targets_1hot[:5])\n",
    "    for step in range(370):\n",
    "        c = np.random.choice(train_targets.shape[0], 2000, replace=False)\n",
    "        batch_data = train_dataset[:,:-1][c]\n",
    "        batch_targets = train_targets[c]\n",
    "        batch_targets=np.array([(x == np.arange(2)).astype(np.float32) for x in batch_targets])\n",
    "        feed_dict = {clean_data:batch_data, target:batch_targets}\n",
    "        l, _ = session.run([loss_, optimize_], feed_dict=feed_dict)\n",
    "        if (step % 10 == 0):\n",
    "            print(\"Minibatch logloss at step\", step, \":\", l)\n",
    "    new_input = layer_1_f.eval(feed_dict={clean_data:combined_dataset})\n",
    "    print(session.run(a_, feed_dict={clean_data:combined_dataset[len(train_targets):],target:test_targets_1hot}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125973, 123)\n",
      "((125973, 201), (22544, 201))\n"
     ]
    }
   ],
   "source": [
    "#saving new train & test data \n",
    "from collections import Counter\n",
    "print(train_dataset.shape)\n",
    "new_train_data=new_input[:train_dataset.shape[0]]\n",
    "new_train_data=np.c_[new_train_data,train_targets]\n",
    "new_test_data=new_input[train_dataset.shape[0]:]\n",
    "new_test_data=np.c_[new_test_data, test_targets]\n",
    "print(new_train_data.shape, new_test_data.shape)\n",
    "# np.savez('out/new_input.npz',train=new_train_data, test=new_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=1,class_weight='balanced',n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.731867870076\n"
     ]
    }
   ],
   "source": [
    "print (cross_val_score(model, new_train_data[:, :-1], train_targets, cv=3, scoring='precision_macro').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_train_data = np.load('out/new_input.npz')['train']\n",
    "new_test_data = np.load('out/new_input.npz')['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(new_train_data[:, :-1], train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(new_test_data[:, :-1])\n",
    "# np.savez('out/preds.npz',preds=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.740684882896\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(preds, test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7736 5097]\n",
      " [ 749 8962]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_targets, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
