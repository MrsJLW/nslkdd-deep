{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [],
   "source": [
    "train_dataset = np.loadtxt(open('Train Set/kddtrain_2class_normalized.csv'), delimiter=',', dtype=np.float32, skiprows=1)\n",
    "train_labels = np.loadtxt(open('Train Set/train_labels.csv'), delimiter=',', dtype=np.float32, skiprows=1)\n",
    "test_dataset = np.loadtxt(open('Test Set/KDDTest+_normalized_2.csv'), delimiter=',', dtype=np.float32, skiprows=1)\n",
    "test_labels = np.loadtxt(open('Test Set/test_labels.csv'), delimiter=',', dtype=np.float32, skiprows=1)\n",
    "train_labels = train_labels - 1\n",
    "test_labels = test_labels - 1\n",
    "# train_indices = np.loadtxt(open('Minimized Train Set/SelectedRows.csv'), delimiter=',', dtype=np.int32, skiprows=1)\n",
    "# train_indices -= 2\n",
    "# train_dataset = train_dataset[train_indices]\n",
    "# train_labels = train_labels[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_feature = len(train_dataset[0])\n",
    "num_labels = 2\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(train_dataset)\n",
    "# train_dataset = scaler.fit_transform(train_dataset)\n",
    "# test_dataset = scaler.fit_transform(test_dataset)\n",
    "# l = float(len(train_dataset)) + float(len(test_dataset))\n",
    "# for i in range(num_feature):\n",
    "#     if len(set(train_dataset[:, i])) > 2:\n",
    "#         s = float(sum(train_dataset[:, i]) + sum(test_dataset[:, i]))\n",
    "#         avg = s / l\n",
    "#         train_dataset[:, i] = (train_dataset[:, i] - (avg / 2)) / 2\n",
    "#         test_dataset[:, i] = (test_dataset[:, i] - (avg / 2)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125973"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_dataset = train_dataset[-6670:, :]\n",
    "valid_labels = train_labels[-6670:]\n",
    "train_dataset = train_dataset[:-6670, :]\n",
    "train_labels = train_labels[:-6670]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119303,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (119303, 41) (119303, 2)\n",
      "Validation set (6670, 41) (6670, 2)\n",
      "Testing set (22544, 41) (22544, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, num_feature)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "# test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print 'Training set', train_dataset.shape, train_labels.shape\n",
    "print 'Validation set', valid_dataset.shape, valid_labels.shape\n",
    "print 'Testing set', test_dataset.shape, test_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, num_feature))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    n_hidden_1 = 200\n",
    "    n_hidden_2 = 100\n",
    "    n_hidden_3 = 10\n",
    "    n_input = num_feature\n",
    "\n",
    "    wh1 = tf.Variable(tf.truncated_normal([n_input, n_hidden_1], dtype=tf.float32))\n",
    "    wh2 = tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], dtype=tf.float32))\n",
    "    wh3 = tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3], dtype=tf.float32))\n",
    "    wo = tf.Variable(tf.truncated_normal([n_hidden_3, num_labels], dtype=tf.float32))\n",
    "    b1 = tf.Variable(tf.truncated_normal([n_hidden_1], dtype=tf.float32))\n",
    "    b2 = tf.Variable(tf.truncated_normal([n_hidden_2], dtype=tf.float32))\n",
    "    b3 = tf.Variable(tf.truncated_normal([n_hidden_3], dtype=tf.float32))\n",
    "    bo = tf.Variable(tf.truncated_normal([num_labels], dtype=tf.float32))\n",
    "\n",
    "    def compute_logit(x):\n",
    "        layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, wh1), b1))\n",
    "        layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, wh2), b2))\n",
    "        layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, wh3), b3))\n",
    "        return tf.matmul(layer_3, wo) + bo\n",
    "\n",
    "    \n",
    "    logits = compute_logit(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + 0.0001 * (tf.nn.l2_loss(wh1) + tf.nn.l2_loss(wh2) + tf.nn.l2_loss(wo))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(compute_logit(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(compute_logit(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 1.95225\n",
      "Minibatch accuracy: 41.6%\n",
      "Validation accuracy: 45.7%\n",
      "Minibatch loss at step 500 : 0.747367\n",
      "Minibatch accuracy: 97.8%\n",
      "Validation accuracy: 97.7%\n",
      "Minibatch loss at step 1000 : 0.473955\n",
      "Minibatch accuracy: 98.2%\n",
      "Validation accuracy: 98.0%\n",
      "Minibatch loss at step 1500 : 0.321225\n",
      "Minibatch accuracy: 98.5%\n",
      "Validation accuracy: 98.1%\n",
      "Minibatch loss at step 2000 : 0.256912\n",
      "Minibatch accuracy: 98.2%\n",
      "Validation accuracy: 98.5%\n",
      "Minibatch loss at step 2500 : 0.207983\n",
      "Minibatch accuracy: 98.8%\n",
      "Validation accuracy: 98.5%\n",
      "Minibatch loss at step 3000 : 0.193433\n",
      "Minibatch accuracy: 98.5%\n",
      "Validation accuracy: 98.8%\n",
      "Minibatch loss at step 3500 : 0.171717\n",
      "Minibatch accuracy: 99.0%\n",
      "Validation accuracy: 98.7%\n",
      "Minibatch loss at step 4000 : 0.153105\n",
      "Minibatch accuracy: 99.0%\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 4500 : 0.140647\n",
      "Minibatch accuracy: 99.3%\n",
      "Validation accuracy: 98.9%\n",
      "Minibatch loss at step 5000 : 0.144159\n",
      "Minibatch accuracy: 98.8%\n",
      "Validation accuracy: 99.0%\n",
      "Minibatch loss at step 5500 : 0.132777\n",
      "Minibatch accuracy: 98.5%\n",
      "Validation accuracy: 98.4%\n",
      "Minibatch loss at step"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "preds = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print \"Initialized\"\n",
    "    for step in xrange(num_steps):\n",
    "        \n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print \"Minibatch loss at step\", step, \":\", l\n",
    "            bac = accuracy(predictions, batch_labels)\n",
    "            print \"Minibatch accuracy: %.1f%%\" % bac\n",
    "            vac = accuracy(valid_prediction.eval(), valid_labels) \n",
    "            print \"Validation accuracy: %.1f%%\" % vac\n",
    "\n",
    "    print \"Minibatch loss at step\", step, \":\", l\n",
    "    print \"Minibatch accuracy: %.3f%%\" % accuracy(predictions, batch_labels)\n",
    "    print \"Validation accuracy: %.3f%%\" % accuracy(valid_prediction.eval(), valid_labels)\n",
    "    preds = test_prediction.eval()\n",
    "    print \"Test accuracy: %.3f%%\" % accuracy(preds, test_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_ = [int(round(x[0])) for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9477,  234],\n",
       "       [4945, 7888]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_labels[:,0], preds_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'describe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-17a4df75882e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'describe'"
     ]
    }
   ],
   "source": [
    "test_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 12833, 0.0: 9711})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(test_labels[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       ..., \n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colabVersion": "0.3.2",
  "colab_default_view": {},
  "colab_views": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
