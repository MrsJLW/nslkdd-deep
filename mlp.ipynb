{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import data as d\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "d.recompute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(predictions, labels):\n",
    "    p = [np.argmax(x) for x in predictions]\n",
    "    labs = [np.argmax(x) for x in labels]\n",
    "    return 100 * precision_score(labs, p, average='macro')\n",
    "def confusion(predictions, labels):\n",
    "    p = [np.argmax(x) for x in predictions]\n",
    "    labs = [np.argmax(x) for x in labels]\n",
    "    return confusion_matrix(labs, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    learning_rate = 0.001\n",
    "    display_step = 1\n",
    "    bacthes_per_epoch = 3000\n",
    "    # Network Parameters\n",
    "    training_epochs = 3\n",
    "    n_input = 1000\n",
    "    n_hidden_1 = 100\n",
    "    n_hidden_2 = 10\n",
    "    n_hidden_3 = 5\n",
    "    n_classes = 5\n",
    "    x = tf.placeholder(tf.float32, [None, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    c_w = tf.placeholder(tf.float32, [250])\n",
    "#     tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    dropout_rate = tf.placeholder(tf.float32)\n",
    "    \n",
    "    def multilayer_perceptron(X, weights, biases):\n",
    "        layer_1 = tf.nn.relu(\n",
    "            tf.add(\n",
    "                tf.matmul(tf.nn.dropout(X, dropout_rate), weights['h1']),\n",
    "                biases['b1'])\n",
    "            )\n",
    "        layer_2 = tf.nn.relu(\n",
    "            tf.add(\n",
    "                tf.matmul(tf.nn.dropout(layer_1, dropout_rate), weights['h2']),\n",
    "                biases['b2'])\n",
    "            )\n",
    "        layer_3 = tf.nn.relu(\n",
    "            tf.add(\n",
    "                tf.matmul(tf.nn.dropout(layer_2, dropout_rate), weights['h3']),\n",
    "                biases['b3'])\n",
    "            )\n",
    "        return tf.add(\n",
    "            tf.matmul(tf.nn.dropout(layer_3, dropout_rate), weights['out']),\n",
    "            biases['out']\n",
    "            )\n",
    "    \n",
    "    \n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'h3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'b3': tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    # Construct model\n",
    "    pred = multilayer_perceptron(x, weights, biases)\n",
    "    prediction = tf.nn.softmax(pred)\n",
    "    # Define loss and optimizer\n",
    "    total_l2_loss = 0\n",
    "    for k, v in weights.items():\n",
    "        total_l2_loss += tf.nn.l2_loss(v)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y) * c_w) + 0.000004 * (total_l2_loss)\n",
    "#     cost = precision(prediction, y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "#     valid_prediction = tf.nn.softmax(multilayer_perceptron(tf_valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0 : 333.122\n",
      "Minibatch precision at step 0 : 18.8398415393\n",
      "Minibatch loss at step 500 : 2.25977\n",
      "Minibatch precision at step 500 : 79.9714285714\n",
      "Minibatch loss at step 1000 : 2.05575\n",
      "Minibatch precision at step 1000 : 86.368006993\n",
      "Minibatch loss at step 1500 : 1.98711\n",
      "Minibatch precision at step 1500 : 87.1942446043\n",
      "Minibatch loss at step 2000 : 1.63916\n",
      "Minibatch precision at step 2000 : 85.6505883497\n",
      "Minibatch loss at step 2500 : 1.57865\n",
      "Minibatch precision at step 2500 : 84.1536618586\n",
      "('Epoch:', '0001')\n",
      "Minibatch loss at step 0 : 1.59277\n",
      "Minibatch precision at step 0 : 84.2852859285\n",
      "Minibatch loss at step 500 : 1.68649\n",
      "Minibatch precision at step 500 : 84.8848878394\n",
      "Minibatch loss at step 1000 : 1.42032\n",
      "Minibatch precision at step 1000 : 86.0909368325\n",
      "Minibatch loss at step 1500 : 1.52438\n",
      "Minibatch precision at step 1500 : 86.3399209486\n",
      "Minibatch loss at step 2000 : 1.47334\n",
      "Minibatch precision at step 2000 : 87.6231269958\n",
      "Minibatch loss at step 2500 : 1.39244\n",
      "Minibatch precision at step 2500 : 87.2881432988\n",
      "('Epoch:', '0002')\n",
      "Minibatch loss at step 0 : 1.40088\n",
      "Minibatch precision at step 0 : 87.4161958569\n",
      "Minibatch loss at step 500 : 1.29431\n",
      "Minibatch precision at step 500 : 88.8495575221\n",
      "Minibatch loss at step 1000 : 1.00319\n",
      "Minibatch precision at step 1000 : 90.8244680851\n",
      "Minibatch loss at step 1500 : 0.808452\n",
      "Minibatch precision at step 1500 : 90.960840816\n",
      "Minibatch loss at step 2000 : 0.88097\n",
      "Minibatch precision at step 2000 : 90.5369586859\n",
      "Minibatch loss at step 2500 : 0.755384\n",
      "Minibatch precision at step 2500 : 92.0512820513\n",
      "('Epoch:', '0003')\n",
      "('Test Precision:', 67.444742998854082)\n",
      "[[5981  867  119  491    0]\n",
      " [ 600 8772  193  142    4]\n",
      " [ 545  227 1474  104   71]\n",
      " [  48  595    5 1902    4]\n",
      " [   0  341    2   38   19]]\n"
     ]
    }
   ],
   "source": [
    "prev_loss = 100000\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch in range(training_epochs):\n",
    "        for i in range(bacthes_per_epoch):\n",
    "            batch_xs, batch_ys, cw = d.train_batch_data(50)\n",
    "            train_feed_dict = {\n",
    "                x: batch_xs,\n",
    "                y: batch_ys,\n",
    "                dropout_rate: 9.0,\n",
    "                c_w: cw\n",
    "            }\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                test_feed_dict = {\n",
    "                    x: batch_xs,\n",
    "                    y: batch_ys,\n",
    "                    dropout_rate: 1.0,\n",
    "                    c_w: cw\n",
    "                }\n",
    "                _, c = sess.run(\n",
    "                    [optimizer, cost], feed_dict=train_feed_dict\n",
    "                )\n",
    "#                 if abs(prev_loss - c) < 0.0001:\n",
    "#                     break\n",
    "#                 prev_loss = c\n",
    "                print \"Minibatch loss at step\", i, \":\", c\n",
    "                batch_precision = precision(prediction.eval(feed_dict=test_feed_dict),batch_ys)\n",
    "                print \"Minibatch precision at step\", i, \":\", batch_precision\n",
    "#                 validation_precision = precision(\n",
    "#                     valid_prediction.eval(feed_dict=test_feed_dict),\n",
    "#                     valid_labels\n",
    "#                 )\n",
    "#                 print(\n",
    "#                     \"Validation precission at step\", step, \":\",\n",
    "#                     validation_precision,\n",
    "#                     end='\\n',\n",
    "#                     flush=True\n",
    "#                 )\n",
    "            else:\n",
    "                _, c = sess.run([optimizer, cost], feed_dict=train_feed_dict)\n",
    "#                 if abs(prev_loss - c) < 0.0001:\n",
    "#                     break\n",
    "#                 prev_loss = c\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1))\n",
    "            \n",
    "    #Test model on test dataset\n",
    "    \n",
    "    tdata = np.load('out/new_input.npz')['test']\n",
    "    a, b = tdata[:, :-1], tdata[:, -1]\n",
    "    b = LabelBinarizer().fit_transform(b-1)\n",
    "    preds = prediction.eval(feed_dict={x:a,y:b,dropout_rate:1.0})\n",
    "    print(\"Test Precision:\", precision(preds, b))\n",
    "    print(confusion(preds, b))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
