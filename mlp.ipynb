{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45927 67343 11656   995    52]\n",
      "[  1   1   3  26 485]\n",
      "[ 1.          1.          1.00413223  1.05165289  2.        ]\n",
      "[45927 67343 11656   995    52]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nilabhra/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/nilabhra/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[  1   1   3  26 485]\n",
      "[ 1.          1.          1.00413223  1.05165289  2.        ]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import data as d\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "d.recompute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(predictions, labels):\n",
    "    p = [np.argmax(x) for x in predictions]\n",
    "    labs = [np.argmax(x) for x in labels]\n",
    "    return 100 * precision_score(labs, p, average='macro')\n",
    "def confusion(predictions, labels):\n",
    "    p = [np.argmax(x) for x in predictions]\n",
    "    labs = [np.argmax(x) for x in labels]\n",
    "    return confusion_matrix(labs, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    learning_rate = 0.001\n",
    "    display_step = 1\n",
    "    bacthes_per_epoch = 10000\n",
    "    # Network Parameters\n",
    "    n_input = 50\n",
    "    n_hidden_1 = 50\n",
    "    n_hidden_2 = 50\n",
    "    n_hidden_3 = 50\n",
    "    n_hidden_4 = 50\n",
    "    n_classes = 5\n",
    "    x = tf.placeholder(tf.float32, [None, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    c_w = tf.placeholder(tf.float32, [200])\n",
    "#     tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    dropout_rate = tf.placeholder(tf.float32)\n",
    "    \n",
    "    def multilayer_perceptron(X, weights, biases):\n",
    "        layer_1 = tf.nn.relu(\n",
    "            tf.add(\n",
    "                tf.matmul(X, weights['h1']),\n",
    "                biases['b1'])\n",
    "            )\n",
    "        layer_2 = tf.nn.relu(\n",
    "            tf.add(\n",
    "                tf.matmul(layer_1, weights['h2']),\n",
    "                biases['b2'])\n",
    "            )\n",
    "        layer_3 = tf.nn.relu(\n",
    "            tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "            )\n",
    "        layer_4 = tf.nn.relu(\n",
    "            tf.add(\n",
    "                tf.matmul(layer_3, weights['h4']),\n",
    "                biases['b4'])\n",
    "            )\n",
    "        return tf.add(\n",
    "            tf.matmul(tf.nn.dropout(layer_4, dropout_rate), weights['out']),\n",
    "            biases['out']\n",
    "            )\n",
    "    \n",
    "    \n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'h3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3])),\n",
    "        'h4': tf.Variable(tf.truncated_normal([n_hidden_3, n_hidden_4])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_4, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'b3': tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
    "        'b4': tf.Variable(tf.truncated_normal([n_hidden_4])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    # Construct model\n",
    "    pred = multilayer_perceptron(x, weights, biases)\n",
    "    prediction = tf.nn.softmax(pred)\n",
    "    # Define loss and optimizer\n",
    "    total_l2_loss = 0\n",
    "#     for k, v in weights.items():\n",
    "#         total_l2_loss += tf.nn.l2_loss(v)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y) * c_w) #+ 0.0001 * (total_l2_loss)\n",
    "#     cost = precision(prediction, y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    init = tf.initialize_all_variables()\n",
    "#     valid_prediction = tf.nn.softmax(multilayer_perceptron(tf_valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0 : 16045.5\n",
      "Minibatch precision at step 0 : 4.08163265306\n",
      "Minibatch loss at step 500 : 249.336\n",
      "Minibatch precision at step 500 : 89.592760181\n",
      "Minibatch loss at step 1000 : 66.209\n",
      "Minibatch precision at step 1000 : 84.0537240537\n",
      "Minibatch loss at step 1500 : 9.37548\n",
      "Minibatch precision at step 1500 : 84.1153255347\n",
      "Minibatch loss at step 2000 : 2.58868\n",
      "Minibatch precision at step 2000 : 87.069124424\n",
      "Minibatch loss at step 2500 : 1.98713\n",
      "Minibatch precision at step 2500 : 83.1708005646\n",
      "Minibatch loss at step 3000 : 1.95953\n",
      "Minibatch precision at step 3000 : 84.5224853011\n",
      "Minibatch loss at step 3500 : 1.98759\n",
      "Minibatch precision at step 3500 : 86.736655281\n",
      "Minibatch loss at step 4000 : 1.82903\n",
      "Minibatch precision at step 4000 : 88.191681736\n",
      "Minibatch loss at step 4500 : 1.62275\n",
      "Minibatch precision at step 4500 : 89.3103448276\n",
      "Minibatch loss at step 5000 : 1.88339\n",
      "Minibatch precision at step 5000 : 86.3111467522\n",
      "Minibatch loss at step 5500 : 1.75857\n",
      "Minibatch precision at step 5500 : 87.5248923768\n",
      "Minibatch loss at step 6000 : 2.05963\n",
      "Minibatch precision at step 6000 : 86.5546001885\n",
      "Minibatch loss at step 6500 : 1.62154\n",
      "Minibatch precision at step 6500 : 88.9242685026\n",
      "Minibatch loss at step 7000 : 1.69919\n",
      "Minibatch precision at step 7000 : 89.3023255814\n",
      "Minibatch loss at step 7500 : 1.5663\n",
      "Minibatch precision at step 7500 : 90.8035714286\n",
      "Minibatch loss at step 8000 : 1.43149\n",
      "Minibatch precision at step 8000 : 89.3968253968\n",
      "Minibatch loss at step 8500 : 1.55636\n",
      "Minibatch precision at step 8500 : 90.5050505051\n",
      "Minibatch loss at step 9000 : 1.46739\n",
      "Minibatch precision at step 9000 : 90.3820816864\n",
      "Minibatch loss at step 9500 : 1.46874\n",
      "Minibatch precision at step 9500 : 90.0911350456\n",
      "('Test Precision:', 61.489272806024985)\n",
      "[[1513  868  274    2 4801]\n",
      " [ 359 8200  240   12  900]\n",
      " [  63  401 1595    5  357]\n",
      " [   0 1303   51   84 1116]\n",
      " [   0  336    1    1   62]]\n"
     ]
    }
   ],
   "source": [
    "prev_loss = 100000\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver()\n",
    "    for i in range(bacthes_per_epoch):\n",
    "        batch_xs, batch_ys, cw = d.train_batch_data(40)\n",
    "        train_feed_dict = {\n",
    "            x: batch_xs,\n",
    "            y: batch_ys,\n",
    "            dropout_rate: 0.5,\n",
    "            c_w: cw\n",
    "        }\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            test_feed_dict = {\n",
    "                x: batch_xs,\n",
    "                y: batch_ys,\n",
    "                dropout_rate: 1.0,\n",
    "                c_w: cw\n",
    "            }\n",
    "            _, c = sess.run(\n",
    "                [optimizer, cost], feed_dict=train_feed_dict\n",
    "            )\n",
    "#                 if abs(prev_loss - c) < 0.0001:\n",
    "#                     break\n",
    "#                 prev_loss = c\n",
    "            print \"Minibatch loss at step\", i, \":\", c\n",
    "            batch_precision = precision(prediction.eval(feed_dict=test_feed_dict),batch_ys)\n",
    "            print \"Minibatch precision at step\", i, \":\", batch_precision\n",
    "#                 validation_precision = precision(\n",
    "#                     valid_prediction.eval(feed_dict=test_feed_dict),\n",
    "#                     valid_labels\n",
    "#                 )\n",
    "#                 print(\n",
    "#                     \"Validation precission at step\", step, \":\",\n",
    "#                     validation_precision,\n",
    "#                     end='\\n',\n",
    "#                     flush=True\n",
    "#                 )\n",
    "        else:\n",
    "            _, c = sess.run([optimizer, cost], feed_dict=train_feed_dict)\n",
    "#                 if abs(prev_loss - c) < 0.0001:\n",
    "#                     break\n",
    "#                 prev_loss = c\n",
    "\n",
    "\n",
    "    #Test model on test dataset\n",
    "    \n",
    "    tdata = np.load('out/new_input.npz')['test']\n",
    "    a, b = tdata[:, :-1], tdata[:, -1]\n",
    "    b = LabelBinarizer().fit_transform(b-1)\n",
    "    preds = prediction.eval(feed_dict={x:a,y:b,dropout_rate:1.0})\n",
    "    print(\"Test Precision:\", precision(preds, b))\n",
    "    print(confusion(preds, b))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
